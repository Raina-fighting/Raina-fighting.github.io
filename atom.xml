<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Raina</title>
  
  <subtitle>Nothing is impossible to a willing heart!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://Raina-fighting.github.io/"/>
  <updated>2019-09-10T03:45:48.615Z</updated>
  <id>http://Raina-fighting.github.io/</id>
  
  <author>
    <name>Yang Raina</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>期刊数据抓取</title>
    <link href="http://Raina-fighting.github.io/2019/09/05/%E6%9C%9F%E5%88%8A%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96/"/>
    <id>http://Raina-fighting.github.io/2019/09/05/期刊数据抓取/</id>
    <published>2019-09-05T10:51:46.000Z</published>
    <updated>2019-09-10T03:45:48.615Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">期刊数据爬取</h1><blockquote><p><strong>本案例系同学共同成果，此部分仅限爬虫代码</strong></p></blockquote><ul><li>《统计研究》于1984年创刊，月刊，是由中华人民共和国国家统计局主管、中国统计学会和中华人民共和国国家统计局统计科学研究所主办的学术性期刊。</li><li>从百度学术上爬取的数据包括了文章的标题、作者、被引量、发文时间、摘要、关键词几个部分</li><li>此数据可研究期刊热点演变、挖掘期刊关注主题或去探究作者的合作关系等，有兴趣的读者可留言探讨</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''加载模块'''</span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">page=np.arange(<span class="number">0</span>,<span class="number">660</span>,<span class="number">10</span>)</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">cookies=&#123;<span class="string">"cookies"</span>:<span class="string">"BAIDUID=2A8356C9A6AF44FECF9A91D6A7F8E5C2:FG=1; PSTM=1555385051; delPer=0; BIDUPSID=838C6450CB0F1BA9A29EE2F1C8A9B28F; BD_CK_SAM=1; H_PS_PSSID=; Hm_lvt_43172395c04763d0c12e2fae5ce63540=1557540625; PSINO=1; BDRCVFR[w2jhEs_Zudc]=mbxnW11j9Dfmh7GuZR8mvqV; BDSVRTM=183; Hm_lpvt_43172395c04763d0c12e2fae5ce63540=1557547102"</span>&#125;</span><br><span class="line"><span class="comment">#headers和cookies查看个人使用的浏览器设置</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">newsData = OrderedDict()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> page:</span><br><span class="line">    href=<span class="string">'http://xueshu.baidu.com/s?wd=%E7%BB%9F%E8%AE%A1%E7%A0%94%E7%A9%B6&amp;pn='</span>+str(i)+<span class="string">'&amp;tn=SE_baiduxueshu_c1gjeupa&amp;ie=utf-8&amp;usm=1&amp;sc_f_para=sc_tasktype%3D%7BfirstSimpleSearch%7D&amp;sc_hit=1'</span></span><br><span class="line">    html = requests.get(href,headers=headers)</span><br><span class="line">    soup = BeautifulSoup(html.content, <span class="string">'html.parser'</span>)</span><br><span class="line">    divs = soup.findAll(<span class="string">'div'</span>, &#123;<span class="string">"class"</span>: <span class="string">"sc_content"</span>&#125;)</span><br><span class="line">    xueshu_href=<span class="string">"http://xueshu.baidu.com"</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(divs)):</span><br><span class="line">        jilu=&#123;&#125;</span><br><span class="line">        head = divs[j].findAll(<span class="string">'h3'</span>,&#123;<span class="string">"class"</span>: <span class="string">"t c_font"</span>&#125;)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment">#标题</span></span><br><span class="line">        titleinfo = head.find(<span class="string">'a'</span>)</span><br><span class="line">        title = titleinfo.get_text().strip().replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        <span class="comment">#网址</span></span><br><span class="line">        url = titleinfo[<span class="string">'href'</span>]</span><br><span class="line">        url=xueshu_href+url</span><br><span class="line">        url_cut=url.split(<span class="string">"%"</span>)</span><br><span class="line">        q=url_cut[<span class="number">2</span>][<span class="number">2</span>:]</span><br><span class="line">        url=<span class="string">"http://xueshu.baidu.com/usercenter/paper/show?paperid=%s&amp;site=xueshu_se"</span>%q</span><br><span class="line">        <span class="comment">#作者</span></span><br><span class="line">        author=<span class="string">""</span></span><br><span class="line">        otherinfo = divs[j].find(<span class="string">"div"</span>,&#123;<span class="string">"class"</span>: <span class="string">"sc_info"</span>&#125;)</span><br><span class="line">        b = otherinfo.find(<span class="string">"span"</span>)</span><br><span class="line">        result=b.findAll(<span class="string">"a"</span>)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> result:</span><br><span class="line">            author=author+p.get_text().strip().replace(<span class="string">"\n"</span>, <span class="string">""</span>)+<span class="string">" "</span></span><br><span class="line">        <span class="comment">#被引量</span></span><br><span class="line">        cite_cont=otherinfo.find(<span class="string">"a"</span>,&#123;<span class="string">"class"</span>:<span class="string">"sc_cite_cont"</span>&#125;).get_text().strip().replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        <span class="comment">#发表时间</span></span><br><span class="line">        time=otherinfo.find(<span class="string">"span"</span>,&#123;<span class="string">"class"</span>:<span class="string">"sc_time"</span>&#125;).get_text().strip().replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        jilu[<span class="string">"link"</span>]=url</span><br><span class="line">        jilu[<span class="string">"time"</span>]=time</span><br><span class="line">        jilu[<span class="string">"title"</span>]=title</span><br><span class="line">        jilu[<span class="string">"author"</span>]=author</span><br><span class="line">        jilu[<span class="string">"cite"</span>]=cite_cont</span><br><span class="line">        <span class="keyword">if</span>(i&gt;<span class="number">0</span>):</span><br><span class="line">            newsData[i+j<span class="number">-1</span>] = jilu</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            newsData[i+j] = jilu</span><br><span class="line">data=pd.DataFrame.from_dict(newsData).T</span><br><span class="line">order = [<span class="string">"title"</span>,<span class="string">"author"</span>,<span class="string">"cite"</span>,<span class="string">"time"</span>,<span class="string">"link"</span>]</span><br><span class="line">data=data[order]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''进详情页获取摘要和关键词'''</span></span><br><span class="line">n=len(data)</span><br><span class="line">keyWords=[]</span><br><span class="line">Abstract=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    url=data.iloc[i][<span class="string">"link"</span>]</span><br><span class="line">    html = requests.get(url,headers=headers)</span><br><span class="line">    soup = BeautifulSoup(html.content, <span class="string">'html.parser'</span>)</span><br><span class="line">    <span class="comment">#摘要和关键词</span></span><br><span class="line">    <span class="keyword">if</span>(soup.find(<span class="string">'p'</span>, &#123;<span class="string">"class"</span>: <span class="string">"abstract"</span>&#125;)==<span class="literal">None</span>):</span><br><span class="line">        Abstract.append(<span class="string">"无"</span>)</span><br><span class="line">        keyWords.append(<span class="string">"无"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        abstract = soup.find(<span class="string">'p'</span>, &#123;<span class="string">"class"</span>: <span class="string">"abstract"</span>&#125;).get_text().strip().replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        Abstract.append(abstract)</span><br><span class="line">        aa=soup.find(<span class="string">'p'</span>, &#123;<span class="string">"class"</span>: <span class="string">"kw_main"</span>&#125;)</span><br><span class="line">        bb=aa.findAll(<span class="string">"span"</span>)</span><br><span class="line">        keywords=<span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> bb:</span><br><span class="line">            keywords+=i.get_text()+<span class="string">" "</span></span><br><span class="line">        keyWords.append(keywords)</span><br><span class="line">data[<span class="string">"abstract"</span>]=Abstract</span><br><span class="line">data[<span class="string">"keywords"</span>]=keyWords</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
       
    
    </summary>
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/categories/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
      <category term="爬虫" scheme="http://Raina-fighting.github.io/categories/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python" scheme="http://Raina-fighting.github.io/tags/python/"/>
    
      <category term="爬虫" scheme="http://Raina-fighting.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>基于新闻联播的文本数据分析</title>
    <link href="http://Raina-fighting.github.io/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://Raina-fighting.github.io/2019/09/04/基于新闻联播的文本数据分析/</id>
    <published>2019-09-04T10:51:00.000Z</published>
    <updated>2019-09-10T08:35:24.731Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center"> 基于新闻联播的文本数据分析</h1> <blockquote><p><strong>本案例系中央财经大学应美玲友情提供</strong></p></blockquote><h2 id="一、研究背景与意义"><a href="#一、研究背景与意义" class="headerlink" title="一、研究背景与意义"></a>一、研究背景与意义</h2><p>有一档节目，在中国几乎无人不知，无人不晓，它就是CCTV的《新闻联播》。自1978年1月1日开始，《新闻联播》每天晚上七点准时在中央电视台及各省级电视台同步播出，时长为30分钟。这档新闻节目聚焦于政治、经济、科技、军事、外交、文化等主题，准确地给全国观众报道最新最及时的新闻。1982年，中央政府更是明文规定了重要的新闻必须在《新闻联播》中首先播出，这一规定揭示了《新闻联播》的重要地位。该节目宗旨为“宣传党和政府的声音，传播天下大事”。</p><p>该档节目播出至今已经有四十余年的历史，它已经深入的渗透到了人们的生活中，甚至改变了人们的生活方式。通过观看《新闻联播》，我们可以了解到国家最新出台的政策，可以对当前国家经济形势有一个大致的了解，可以真正做到足不出户便已了解国家大事。《新闻联播》已经成为一个风向标和指明灯，它给老百姓们的生活带来了方向：对于投资者来说，观看《新闻联播》有助于他们敏锐的发现各种投资机会；对于大学生来说，观看《新闻联播》可以帮助他们明晰行业的发展情况，从而有助于做出就业的选择；对于政府官员来说，《新闻联播》在很大程度上明确了他们未来的工作方向，若提前得知并做好充足准备，有助于他们的政绩表现并提高晋升的概率；甚至对于外国友人或者外企来说，想要深入了解中国，《新闻联播》是必不可少的了解渠道之一。</p><p>由此可以看到，《新闻联播》对于我们所有人来说价值都是巨大的，它与我们的切身利益息息相关，密不可分。但是，却很少有人去真正研究该档新闻节目，我认为其中主要有以下两方面原因：第一，因为《新闻联播》每天都会准时播出，人们已经司空见惯习以为常，反倒忽略了其中巨大的价值，就像空气对于人类的作用一样；第二，《新闻联播》是以文本数据的形式存在，不像传统的数值数据，方便统计分析和数学建模。</p><h2 id="二、样本选取与数据来源"><a href="#二、样本选取与数据来源" class="headerlink" title="二、样本选取与数据来源"></a>二、样本选取与数据来源</h2><p>本文所使用的新闻联播文本数据来自于Tushare金融大数据社区 <a href="https://tushare.pro/" target="_blank" rel="noopener">1</a>，获取了2006年6月15日至2019年7月2日的全部新闻联播文本数据，一共有77561条观测。每一天平均有10至25条不等的新闻。每一条观测有三列，分别是日期（date）、标题（title）和内容（content）。例如，表1是本文样本数据集中的两条观测。本文主要是对content的文本数据进行分析。</p><center>表 1 部分样本数据</center><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image001.png" alt="img" style="zoom:67%;"> <h2 id="三、数据描述性分析"><a href="#三、数据描述性分析" class="headerlink" title="三、数据描述性分析"></a>三、数据描述性分析</h2><p>由于文本数据没有具体的数值，所以只能通过Python对新闻联播文本数据进行分词，然后做词频统计。利用得到的词频表进行后续一系列的描述分析，从而将新闻联播的文本数据转为可视化的图表。</p><h3 id="（一）新闻联播词云图"><a href="#（一）新闻联播词云图" class="headerlink" title="（一）新闻联播词云图"></a>（一）新闻联播词云图</h3><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image003.png" alt="img" style="zoom: 67%;"><center>图 1 新闻联播词云图</center><p>从图1中可以看到，《新闻联播》中最经常提到的词语是发展、中国、经济、合作、习近平、建设、企业、美国、文化等，这些高频词语的出现也正好契合了《新闻联播》的主旨——即聚焦于国内经济发展建设、外交、文化等方面。</p><h3 id="（二）各省份受关注情况"><a href="#（二）各省份受关注情况" class="headerlink" title="（二）各省份受关注情况"></a>（二）各省份受关注情况</h3><p>通常情况下，某个省份越受到中央政府的重视和关注，那么它在新闻联播中被提及的次数就越多。基于这样的规律，在统计各个省份和直辖市的词频分布之后，绘制了整体情况下的各省份在新闻联播中出现的频数柱状图和某些省份被提及频数随着时间的变化图，如图2至图12所示。</p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image005.png" alt="img" style="zoom:60%;"><center>图 2 各省份被提及频数柱状图</center><p>从图2中可以看到，从2006年至2019年这14年时间里，上海被提到的次数最多，超过了一万次；其次是香港和北京，分别被提到了7200次和6640次。这三个城市在中国的地位也是相当重要的：北京是中国的首都；上海是著名的金融中心；香港则是重要的外汇交易中心和人民币离岸交易中心。被提到次数最少的三个省份分别是山西、青海和宁夏，可以认为这三个省份得到的关注相对较少。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image008.png" alt="img"></p><center>图 3 上海被提及频数的时间分布图</center><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image010.png" alt="img"></p><center>图 4 北京被提及频数的时间分布图</center><p> 上海和北京都是中国最重要的两个城市。从图3中可以看到，每年上海被提及的次数都在500次以上，期间在2010年达到峰值，这是因为2010年在上海举行了世博会。从图5中看到，2006年至2008年，北京被提到的次数逐年增多，并在2008年达到峰值，这是因为2008年北京成功举办了奥运会；但是2008年之后，北京被提到的次数逐年下降，但平均值也在300次左右。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image012.png" alt="img"></p><center>图 5 内蒙古被提及频数的时间分布图</center><p> 内蒙古作为中国的一个自治区，主要生活着蒙古族的人民群众。从图5中可以看到，2007年和2017年内蒙古被提到的次数达到了两个峰值，这主要是对应着内蒙古自治区成立的60和70周年。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image014.png" alt="img"></p><center>图 6 香港被提及频数的时间分布图</center><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image016.png" alt="img"></p><center>图 7 澳门被提及频数的时间分布图</center><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image018.png" alt="img"></p><center>图 8 台湾被提及频数的时间分布图</center><p>香港、澳门和台湾作为曾经的殖民地，回归祖国后自然受到中央政府的许多关注。从图6中可以看到，2007年和2017年香港出现在新闻联播的次数达到峰值，其中2007年香港被提到的次数多达1513次，这是因为2007年和2017年分别是香港回归祖国的十周年和二十周年。从2007年提及香港1513次，可以看出我国对回归后的香港十分重视，以及对香港回归喜悦之极。澳门的情况也是类似的，图7中2009年和2014年分别对应着澳门回归祖国的十周年和十五周年，这两年新闻联播提及澳门的次数也达到了平时的两倍。图8中台湾在2007年和2009年被新闻联播提到的次数比较多，这是因为在这两个时间段内，台湾的政局比较动荡。一直以来，台湾问题就比较具有政治敏感性，作为官方媒体的新闻联播自然对台湾岛保持着较高的关注度。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image020.png" alt="img"></p><center>图 9 四川被提及频数的时间分布图</center><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image022.png" alt="img"></p><center>图 10 青海被提及频数的时间分布图</center><p> 四川省和青海省在历史上都曾经发过地震。从图9中可以看到，2008年5月12日汶川发生8.0级地震，于是2008年四川省被新闻联播提及1552次；2010年4月14日青海玉树发生7.1级地震，当年新闻联播提及到青海374次。从图9和图10中可以看到，中央政府对地震灾区是及其关注和重视的，尤其是汶川地震，因为汶川地震是中国人民共和国成立以来最严重的一次地震。在汶川地震后的几年里，中央政府一直致力于灾后重建，从2009年到2014年四川省一直保持着较高的提及频率便可以看出来。结合图1来看，从2006年-2019年，四川总体上被提到的次数仅低于上海香港和北京。现如今，四川省会成都市已经成功跻身新一线城市，可见汶川地震带给四川的影响在中央政府的努力下已经消失殆尽。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image024.png" alt="img"></p><center>图 11 新疆被提及频数的时间分布图</center><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image026.png" alt="img"></p><center>图 12 西藏被提及频数的时间分布图</center><p> 新疆和西藏是我国五个自治区之二，主要生活着维吾尔族和藏族人民。新疆自治区于1955年成立，西藏自治区则于1965年成立。2015年分别对应着自治区成立的60和50周年，所以被新闻联播提到的次数达到了一个小高峰。从图11中可以看到，2009年新疆被提到了843次，这是因为当年新疆发生了极其严重的震惊全国的犯罪事件。图12中显示着2009年西藏被提到了985次，这是因为当年确定了西藏百万农奴解放纪念日。</p><h3 id="（三）中国外交情况"><a href="#（三）中国外交情况" class="headerlink" title="（三）中国外交情况"></a>（三）中国外交情况</h3><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image028.png" alt="img"></p><center>图 13 各国被提及的百分比</center><p>外交一直是新闻联播的一个重要板块。通过词频分布，我们可以了解到中国和哪些国家往来密切。从图13中可以看到，新闻联播里提到美国的占比最多，达到22.81%。这说明我国还是十分重视中美关系，因为中国和美国一个是人口大国，一个是高度发达的超级强国，两个国家之间已经深深地利益相关。除此之外，中国和俄罗斯、日本的联系也十分的密切。叙利亚和伊朗因为战乱长期动荡不安，引起了我国的关注。</p><h3 id="（四）各高校被提及情况"><a href="#（四）各高校被提及情况" class="headerlink" title="（四）各高校被提及情况"></a>（四）各高校被提及情况</h3><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image030.png" alt="img"></p><center>图 14 各高校被提及的百分比</center><p>通过统计新闻联播中提到的各高校的次数，可以总结出我国比较重视的一些高校，从而可以给刚高考完的学生填志愿提供一定的依据。从图14中可以看到，清华北大被提到的占比和为41.07%，这说明我国最顶尖的两所学府，受到中央非一般的重视。中国人民大学、北京师范大学和复旦大学紧随其后。一个很明显的现象是，在图14中，几乎所有的大学都是理工类或综合类，只有中国政法大学一个例外。这也说明了中央政府对综合类和理工类的高校的重视程度要远高于偏文科类的高校。</p><h3 id="（五）新闻热词分布情况"><a href="#（五）新闻热词分布情况" class="headerlink" title="（五）新闻热词分布情况"></a>（五）新闻热词分布情况</h3><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image032.png" alt="img"></p><center>图 15 一带一路分布情况</center><p>​       自从习近平主席在2013年提出“新丝绸之路经济带”后，“一带一路”这个词的热度便一直高居不下。从图15中可以看到，2017年“一带一路”这个词被提到了1463次，虽然后续呈下降趋势，但仍然保持着超高的提及频率。这也表明政府对“一带一路”建设高度重视。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image034.png" alt="img"></p><center>图 16 5G分布情况</center><p>从图16中可以看到，5G这个词语在2014年开始出现，但是一直到2018年，5G被提到的次数都十分的少。然而在2019年，5G出现的次数却有了一个质的飞跃，从31次迅速上升到147次。这是因为2019年华为宣布掌握了5G技术。这一消息迅速引起了政府的注意，可见我国对科技的重视程度。根据图16可以合理的预测，在未来的几年里，5G将会变得越来越重要。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image036.png" alt="img"></p><center>图 17 转型、升级分布情况</center><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image038.png" alt="img"></p><center>图 18 创新、改革分布情况</center><p>从图17、18中可以看到，自从党的十八大之后，经济转型、行业升级、改革创新这些词在新闻联播中被提起的越来越频繁，其中都纷纷在2016年达到了最高点。党中央提出经济新常态，只有对经济转型，对行业升级，对科技改革创新才能达到这一目标。</p><h3 id="（六）中国宏观经济情况"><a href="#（六）中国宏观经济情况" class="headerlink" title="（六）中国宏观经济情况"></a>（六）中国宏观经济情况</h3><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image040.png" alt="img"></p><center>图 19 三大产业被提及情况</center><p>从图19中可以看到，新闻中提到农业的次数远远高于工业和服务业，这说明中央政府还是最关注农业的发展情况，这也和目前的国情相符，当下中国还是以第一产业为主。在2012年之前，农业和工业存在着一个此消彼长的负相关关系，这可能是因为年轻人纷纷选择外出务工而非务农，使得工业增长而农业下降；2012年之后，工业和农业呈现同增同长的关系，这可能是因为工业的发展也提高了农业的效率，从而间接促进农业的发展。</p><p><img src="/2019/09/04/%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/image042.png" alt="img"></p><center>图 20 投资、消费、出口被提及情况</center><p>投资、消费和出口被称为拉动经济增长的三驾马车。从图20中看到，投资一直以来都是我国经济增长的主要因素，期间虽然不停的起伏震荡，但中央仍给予了足够的重视。在2009年，全世界遭受金融危机时，我国开始强调消费、内需来拉动经济的增长。三驾马车的格局已经从2006年的投资、出口、消费转变为2019年的投资、消费、出口。</p>]]></content>
    
    <summary type="html">
    
        
    
    </summary>
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/categories/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
      <category term="可视化" scheme="http://Raina-fighting.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>世界银行借贷款情况分析</title>
    <link href="http://Raina-fighting.github.io/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/"/>
    <id>http://Raina-fighting.github.io/2019/09/03/世界银行借贷款情况分析/</id>
    <published>2019-09-03T02:22:17.000Z</published>
    <updated>2019-09-09T11:45:23.285Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">世界银行借贷款情况分析</h1><blockquote><p> <strong>本案例得到中央财经大学周萌和姚沛君友情支援</strong></p></blockquote><p>本次案例从宏观至微观对数据集中的重要信息进行分析：首先从宏观层面对全球各个地区的借贷款情况及其资金投入方向进行考察，随后选取中国作为分析对象，对本国贷款项目资金来源于主要用途进行分析，最后选取国内贷款资金耗费量最大的汶川震后重建项目进行分析。</p><h2 id="全球借贷款情况分析"><a href="#全球借贷款情况分析" class="headerlink" title="全球借贷款情况分析"></a>全球借贷款情况分析</h2><ul><li>各地区的不同贷款项数目差异情况</li></ul><p>其中经济发展水平较高的拉丁美洲加勒比地区、欧洲与中亚地区申请贷款项目中涉及公共管理以及法律相关的数目较多；反之，经济发展水平较低的非洲地区以及中东地区所涉及到的农林业相关项目数量较多；南亚地区在教育方面的项目数量占其总申请项目数目的比值要远远高于其他国家与地区，非洲以及东亚地区各类项目数量占比较为均衡。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image001.png" alt="png">                                         </p><center>图1.1 不同地区项目主要部门（sector）购买次数占比情况</center><p>对比各地的各个项目采购数目，其中拉丁美洲与加勒比地区签署的项目最多，非洲的项目数量位居第二，项目最少的为中东地区南非地区。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image002.png" alt="png"></p><center>图1.2 不同地区项目主要部门（sector）购买次数绝对值情况</center><ul><li>不同地区的采购目录（Procurement Category）购买数量呈现差异性</li></ul><p>观察不同地区采购目录可以反应不同地区贷款资金的流向，可以观察到各个地区购买数量最多的均为咨询类服务，所涉及的咨询类服务包括管理建议、投标文件协助、知识产权转移、教育类服务、软件类服务、统计类服务以及安保服务等接近30项服务，可以看出随着经济社会的进一步发展，社会形态变得越来越复杂，对于不同咨询类服务的需求日益增长，成为一个国家与地区发展过程中不容忽视的一项内容。各地区购买数量位居第二的为各类商品，商品经济的繁荣，使得商品的进出口成为一个国家不可或缺的经济组成部分。除此之外的土木工建以及非咨询类项目支出均占比较小，且总金额数目也较少。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image003.png" alt="png"></p><center>图1.3 不同地区采购目录购买数量占比情况</center><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image004.png" alt="png"></p><center>图1.4.1 不同地区采购目录购买数量绝对值情况</center><p>除此之外，采购方法上（Procurement Method）也存在着一定的差异，其中数量排名位居前三位的分别为工业、资源、以及互联网项目投标，从中也可以反应目前世界银行签署的贷款项目侧重投资的几大方面：第二产业的发展仍然是众多贷款申请国家注重的方面。与此同时，各国的能源以及资源问题也受到普遍关注。随着互联网时代的高速发展，互联网相关产业的投入也是必不可少的。   </p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image005.png" alt="png"></p><center>图1.4.2购买项目方式数量差异情况</center><ul><li>不同区域接受贷款所涉及的项目主要部门（Major Sector）差异情况</li></ul><p>具体查看不同地区贷款所涉及的项目主要部门，观察到不同地区的资金投资结构呈现差异性，其中东亚、中东以及北非地区资金投资结构较为相似，对于交通、用水以及能源的投资较多；欧洲、中亚以及南亚的投资结构较为相似，都在能源方面投资最多，除了对于交通以及用水的投入，还增加了对于法律方面的投资。非洲地区整体的投资结构与其他地区呈现明显的差异，其在各个方面的投资都出于一个较高水平，不像其他区域都只在一个或两个项目上有比较大的投资，尤其是在通信以及用水方面的投资明显高于其他地区，这也是由于非洲地区经济发展各个方面都较为落后，很多基础配套设施亟待更新换代，社会问题较为复杂，相比于欧洲、北美等发达国家占据的地区，非洲在各个方面都需要大量资金的投入都比较大。 </p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image006.png" alt="png">  </p><center>图1.5 不同区域的项目主要部门（Major Sector）金额差异情况</center><ul><li>不同区域采购目录（Procurement Category）金额差异情况</li></ul><p>不同地区购买的商品或服务类型呈现出与项目主要部门投资类似的结构特点，相对于其他经济、社会发展较为完善的地区，投资方向趋向单一化，体现出重点，而非洲由于整体出于较为落后的水平，因此对于各类别的服务都投入了较大的资金，尤其是在咨询类服务中投入的资金最多。</p><p>进一步对比采购项目的数量与所花费金额：总体上各个地区咨询类项目购买的次数最多，但除去非洲地区，其余国家所需要资金最多的均为土建项目。土建项目本身需要大量的资金投入，虽然涉及到的项目数量并不多，但需要的贷款金额是巨大的，相比之下，咨询类服务项目繁多并且部分涉及知识产权类的服务，这部分服务涉及经济、社会发展的各个方面，因此需求量较大，但每一笔需要的金额相比于土建项目要小得多。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image007.png" alt="png"></p><center>图1.6 不同地区采购项目金额差异情况</center><ul><li>供给国（Supplier Country）投资差异情况</li></ul><p>选取提供贷款金额最多的前十个国家，观察得到这些国家提供的服务类型呈现差异性，其中越南提供的服务次数总和最多，超过200次，涉及最多的项目是能源与采矿方面的服务；提供服务数目总和位居第二的为巴西，主要同农林、健康以及社会服务类投资；萨尔瓦多与危地马拉主要提供公共管理、法律等方面的服务；中国则主要提供用水、环保等方面的服务。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image008.png" alt="png"></p><center>图1.7 提供贷款金额排名前十国家投资差异情况</center><ul><li>贷款总金额时间序列分析</li></ul><p>观察 2015 年世界银行的有关项目，签订合同金额随签署时间变化的情况，可以发现合同金额大都集中在100,000,000 美元以下，其中最高金额签订于2014 年 7 月 24 日，关于在非洲建设区域通信基础设施项目，该项目服务区域较广，且通信材料较为昂贵，在项目初期投入较多是可以理解的。另外，大多数项目签订在 2014 年后半年及 2015 年，其中 12 月份总金额最高，而 2013 年及之前签订的合同数不足 1%，属于长期执行的项目工程只有少数。结合分面图，12 月份签订的合同总金额在当年之中相对较高，可能与部门运作情况相关，年底的一些项目可能结束考察步入执行。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image009.png" alt="png"></p><center>图1.8  2010-2015年贷款总金额变动情况</center><ul><li>借款国与供应国地域分布</li></ul><p>观察全球范围内贷款申请国的分布情况，可以明显观察到贷款金额较高的国家集中分布在非洲，这与非洲现实的经济发展水平是密切相关的，相对落后的生产发展水平使得非洲各国急需依靠世界银行二点贷款来进行大量的投资与建设活动；另外中国作为发展中国家的代表，贷款总金额也较高，这一系列现象也侧面反应了世界银行设立贷款项目额初衷，即为帮助发展落后以及各发展中国家实现反贫穷的政策，推动全球经济的健康发展。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image010.png" alt="png"></p><center>图1.9.1 贷款申请国分布情况</center><p>注：图1.9.1与1.9.2为采用Echart绘制的交互式地图</p><p>在贷款供应国中，中国的所提供的贷款总金额远高于世界其他国家地区，并且对应上图中中国贷款数目大小，可以得知中国所申请的贷款项目一部分用于本国的投资建设活动，可见随着中国经济的发展、综合国力的进一步提升，中国在世界银行中占有重要地位，在注重自身经济发展的同时，也着重于在世界其他国家进行投资建设活动。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image011.png" alt="png"></p><center>图1.9.2  贷款供应国分布情况</center><h2 id="中国借贷款情况分析"><a href="#中国借贷款情况分析" class="headerlink" title="中国借贷款情况分析"></a>中国借贷款情况分析</h2><ul><li>中国2009-2015年获得贷款情况</li></ul><p>观察中国2009-2015年在世界银行的总贷款金额变动曲线，可以发现其中2009年贷款金额最高，总额超过16亿美元。当年贷款资金投建的项目共涉及63项，其中安徽省公路修复改造工程、福州南台岛城市发展项目、福建公路部门投资、贵阳交通工程四项投资均超过100万美元，金额总和占当年总投资的45.4%。</p><p>2010-2014年各年贷款金额基本在10亿美元的规模上浮动，在2015年贷款金额出现大幅缩减，仅有4亿美元，该年截止日期前共涉及57个项目，投资金额最高的为昆明城市轨道交通项目。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image012.png" alt="png"></p><center>图2.1 中国2009-2015年在世界银行的总贷款金额变动曲线</center><ul><li>中国贷款金额排名前十的项目</li></ul><p>2009-2015年中，筛选投资总金额排名前10的项目，可以观察到投资金额最高的为汶川灾后重建项目，接下来依次为南光铁路项目、宁夏公路工程、安徽省公路修复改造工程、贵阳广州铁路工程、南昌城市轨道交通项目、福州南台岛城市发展项目、湖北宜巴高速公路、福建公路部门投资、石正铁路项目。其中，灾后重建项目本身十分重大，历时较长、规模较大、投资需求量不可避免地高于一般性项目，除此之外，投资金额较高的均为道路交通方面地建设项目，这与近些年来，我国的公路交通发展迅速的现实情况相符，其中高铁发展技术位居世界前列。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image013.png" alt="png">   </p><center>图2.2 中国贷款金额排名前十项目总金额</center><ul><li>中国贷款项目涉及项目主要部门（Major Sector）变动情况</li></ul><p>观察2009-2015年各年的贷款项目类别，可以体现当年投资着重点所在，体现近几年来国家发展重心以及方向所在。可以观察到各年中用于交通以及用水的贷款项目占比最大，尤其是交通方面的投入在近些年来一直是最高的。而娱乐项目在2009-2014年中的各年占比呈现下降的趋势，并在2015年有所“回温”。相比之下，农业、财政、健康、工业、教育、以及法律等相关项目的各年投资占比均较小，可以看出近些年来中国各类项目投资组合结构并没有发生重大变化，仍然以交通设施建设为投资重心，并兼顾其他各项均衡发展。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image014.png" alt="png"></p><center>图2.3  2009-2015年中国贷款项目涉及主要部门变动情况</center><ul><li>国外对中国提供贷款资金情况</li></ul><p>中国各项贷款资金来源主要分为本国资金与国外资金，其中国外资金（含中国香港特别行政区）共计约1540万美元，占贷款总金额的2.0%，其余资金均来自中国在世界银行的认股资金。为中国提供贷款的其余国家共有包括德国在内的14个国家，其中德国提供的贷款金额最高，澳大利亚与加拿大分别位居第二、第三。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image015.png" alt="png"></p><center>图2.4 国外对中国提供贷款资金情况</center><ul><li>重要国外贷款资金来源分析 </li></ul><p>关注提供贷款金额排名前五的国家所投入资金的具体流向，可以观察到一些国家所提供的投资项目比较单一，例如澳大利亚、德国两国几乎全部投资都用于交通建设，新加坡的投资也主要集中在工业以及交通，反之，以加拿大与美国为代表的国家投资项目较为多样化，涉及娱乐、财政、健康、用水等各个方面，但两国对于个项的投资额均较小。值得注意的是，国外投资项目中都没有涉及教育相关项目，我国的教育投资来源还是主要依靠本国的大量投资。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image016.png" alt="png"></p><center>图2.5 提供贷款金额排名前五的国家投资情况</center><h2 id="汶川重建项目分析"><a href="#汶川重建项目分析" class="headerlink" title="汶川重建项目分析"></a>汶川重建项目分析</h2><p>2008年5月12日的汶川大地震牵动着全中国以至于全世界人们的心，灾后重建工作也受到了各方的高度重视。在2015年度，中国世界银行中的贷款项目共涉及136项，其中用于汶川灾后重建的共有15项目，总金额占比最大。该部分重点分析用于汶川灾后重建贷款的相关信息。</p><ul><li>2009-2015年汶川重建项目贷款金额变动</li></ul><p>下图显示了2009年至2015年中国用于汶川灾后重建的世界银行贷款总金额变动情况。前期的灾后重建需要大量的资金投入，因此可以观察到2009年至2011年贷款金额呈现急剧增长的趋势，并于2011年达到最高金额2亿美元，随后随着灾后重建的逐步完善，资金投入的需求逐渐下降，贷款金额也逐年减少，2014年与2015年两年贷款金额基本持平。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image017.png" alt="png"></p><center>图3.1 2009-2015年汶川重建项目贷款金额变动情况</center><ul><li>其他国家提供贷款情况</li></ul><p>中国2009年至2015年用于汶川灾后重建的世界银行贷款资金来源主要分为本国资金与国外资金。其中国外资金约104万美元，占总贷款金额的2.8%。如下图所示，所涉及到的三个借款国家分别为巴西、加拿大以及美国，其中巴西借款金额最高，共计600万美元，美国次之，加拿大金额最小。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image018.png" alt="png"></p><center>图3.2 其他国家提取贷款情况</center><ul><li>汶川重建贷款涉及主要部门</li></ul><p>世界银行拨付贷款的项目主要部门也可以反应汶川灾后重建项目的贷款后期走向。由于灾后道路运输系统受到严重毁坏，因此在道路运输新建项目中需要大量资金投入，可以观察到用于交通建设的贷款金额占比最多；用于健康以及用水方面的资金差异不大，每项金额约占交通运输项目金额的一半。用于教育的资金最少，相对于其余三个项目，可见教育的灾后重建对于资金的需求并没有其余三项大。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image019.png" alt="png"></p><center>图3.3 汶川重建项目各部门资金分配情况</center><p>下图反应灾后贷款中各项目出现的次数，其中涉及健康的项目最多，交通与用水方面的项目次数次之，教育类项目次数最少。对比四个项目的资金以及项目次数分配情况可以观察到教育交通这两项的特点较为鲜明：交通类项目数量较少，但每笔需要的资金投入量较大，因此总金额最高，反之，健康类项目数量较多，需求量较大，但每一个项目的投入不及道路运输的大额投资，因此总金额较少。相比之下，教育类项目不论在数目还是金额上都占比最少。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image020.png" alt="png"></p><center>图3.4 汶川重建贷款各部门项目个数</center><ul><li>2009-2015年贷款主要部门资金分配变动情况</li></ul><p>2009年至2015年各年四个项目的各年占比情况如下图所示，可以观察到灾后第一年中，基本生活资源亟待保障，因此2009年的贷款资金基本全部投入到用水项目当中，另外灾区的保障也十分重要，在灾后的头三年中，健康项目的资金投入占比较大，随着时间推移，健康问题得到了显著改善因此在这一项中的投入也逐渐减少。教育项目的投入在前期投入较大，正常运营之后的投入较少了。在解决了灾后的基本生活问题之后，道路运输系统的投资占比逐年增大，在后期远远超过其余三个项目的资金投入。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image021.png" alt="png"></p><center>图3.5 2009-2015年各部门项目计数情况</center><p>从各年的不同项目投入资金数目也可以观察到各项目的运作特点，相对于其余三项，交通项目的重建体现了其资金需求量大、筹建周期较长的特点。</p><p><img src="/2019/09/03/%E4%B8%96%E7%95%8C%E9%93%B6%E8%A1%8C%E5%80%9F%E8%B4%B7%E6%AC%BE%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90/image022.png" alt="png"></p><center>图3.6 2009-2015年各部门资金分配情况</center>]]></content>
    
    <summary type="html">
    
        
    
    </summary>
    
      <category term="结构化数据" scheme="http://Raina-fighting.github.io/categories/%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="R" scheme="http://Raina-fighting.github.io/tags/R/"/>
    
      <category term="可视化" scheme="http://Raina-fighting.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>英超球员进球数影响因素分析</title>
    <link href="http://Raina-fighting.github.io/2019/09/02/%E8%8B%B1%E8%B6%85%E7%90%83%E5%91%98%E8%BF%9B%E7%90%83%E6%95%B0%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/"/>
    <id>http://Raina-fighting.github.io/2019/09/02/英超球员进球数影响因素分析/</id>
    <published>2019-09-02T10:51:00.000Z</published>
    <updated>2019-09-09T09:33:12.133Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">英超球员进球数影响因素分析</h1>## 一、背景介绍<p>2018世界杯闭幕不久，引起全民足球热潮。英格兰足球超级联盟简称英超，是英格兰足球总会属下的职业足球联赛，欧洲五大联赛之一，由20支球队组成。英超凭借快节奏、竞争激烈、强队众多，成为世界上最受欢迎的体育赛事。本案例通过分析英超球员基本信息和球场表现的情况，找到影响其进球数的因素进行建模。</p><h2 id="二、数据来源与说明"><a href="#二、数据来源与说明" class="headerlink" title="二、数据来源与说明"></a>二、数据来源与说明</h2><p>本案例使用数据为2012到2013赛季的英超赛事数据。数据集包含16支球队的166名球员在此赛季的场上表现和下个赛季的进球数。数据说明表见表1。</p><center>表1：英超数据说明表</center>![png](英超球员进球数影响因素分析/image001.png)<h2 id="三、探索性分析"><a href="#三、探索性分析" class="headerlink" title="三、探索性分析"></a>三、探索性分析</h2><h3 id="（一）因变量分析"><a href="#（一）因变量分析" class="headerlink" title="（一）因变量分析"></a>（一）因变量分析</h3><p>1.进球数分布</p><p>如图1，下个赛季即2013-2014赛季贡献了500个进球。其中，大部分球员进球数较少甚至为0，只有个别球员能力较强球队中担当突出，进球机会多进球数多。其中，苏亚雷斯达到31个，斯图里奇为21个，亚亚·图雷紧随其后20个。</p><p><img src="/2019/09/02/%E8%8B%B1%E8%B6%85%E7%90%83%E5%91%98%E8%BF%9B%E7%90%83%E6%95%B0%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/image002.png" alt="png"></p><center>图1：下一赛季进球数分布条形图</center>2.进球数与场上位置<p>本案例166名球中有86名中场，50名后卫，30名前锋球员。在2012-2013赛季共贡献了624个进球。其中，前锋的人均进球数和最大进球数最高，30名前锋共同贡献了264个进球，占42.3%。进球数最高的前锋是曼联的范佩西；最高的中锋是阿斯顿维拉的本特克。</p><p><img src="/2019/09/02/%E8%8B%B1%E8%B6%85%E7%90%83%E5%91%98%E8%BF%9B%E7%90%83%E6%95%B0%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/image003.png" alt="png"></p><center>图2：本赛季各场上位置进球数</center>3.进球数与年龄段<p>足球是高对抗项目，对运动员身体素质要求较高，从数据集中可知，运动员年龄分布在20-40之间，而高于35岁的只有两名球员。结合图3，年龄段在25-30之间的球员进球数最高，其中一半的前锋球员处在这个年龄段，该年龄段属于前锋球员创造价值的最佳阶段。30-40年龄段平均进球数最高，其中中场的贡献最高，30-40岁是中场球员蓄积经验后的成熟时期。</p><p><img src="/2019/09/02/%E8%8B%B1%E8%B6%85%E7%90%83%E5%91%98%E8%BF%9B%E7%90%83%E6%95%B0%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/image006.png" alt="png"></p><center>图3：不同年龄段进球数</center>4.本赛季与下赛季进球数<p>​        球员在连续的赛季中的表现是很相似的，如图4，本赛季进球数多的球员在下一个赛季同样表现优秀。如利物浦的苏亚雷斯。但也有少数球员两个赛季表现存在较大差异，如范佩西，本赛季进球最多，下个赛季缩减到一半以下。范佩西年龄为30，在球员中已经属于比较高的，更是30岁及以上仍在打前锋的少数球员之一。此外，也可以看出，前锋位置对进球数平均贡献最大。</p><p><img src="/2019/09/02/%E8%8B%B1%E8%B6%85%E7%90%83%E5%91%98%E8%BF%9B%E7%90%83%E6%95%B0%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/image008.png" alt="png"></p><center>图4：球员下一赛季与本赛季进球数的相关图</center>### （二）变量相关性分析<p>选择与运动员场上表现，分为射门、亮点、犯规表现，相关的22个指标，相关系数矩阵图如图5。部分变量间存在较强的相关性，且变量间可能存在一些结构性特征。</p><p><img src="/2019/09/02/%E8%8B%B1%E8%B6%85%E7%90%83%E5%91%98%E8%BF%9B%E7%90%83%E6%95%B0%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/image011.png" alt="png"></p><center>图5：相关系数矩阵图</center><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##探索性分析</span></span><br><span class="line"><span class="comment">#进球数分布</span></span><br><span class="line">summary(mdata$下一年进球)</span><br><span class="line">barplot(table(mdata$下一年进球),ylab=<span class="string">"下一年进球"</span>,col=<span class="string">"lightblue"</span>,main=<span class="string">""</span>)</span><br><span class="line"><span class="comment">#场上位置与总进球数</span></span><br><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">ggplot(mdata,aes(x=factor(位置),y=进球))+</span><br><span class="line">  geom_bar(stat=<span class="string">"identity"</span>,fill=<span class="string">"lightblue"</span>)+</span><br><span class="line">  xlab(<span class="string">"位置"</span>)+ylab(<span class="string">"本赛季总进球数"</span>)</span><br><span class="line"><span class="comment">#年龄与总进球数</span></span><br><span class="line">mdata$年龄分段=cut(mdata$年龄,c(<span class="number">19</span>,<span class="number">25</span>,<span class="number">30</span>,<span class="number">40</span>))<span class="comment"># 48 84 34 </span></span><br><span class="line">ggplot(mdata,aes(x=年龄分段,fill=位置,y=进球))+theme_bw()+</span><br><span class="line">  theme(panel.grid.major=element_blank(),panel.grid.minor=element_blank())+</span><br><span class="line">  geom_bar(stat=<span class="string">"identity"</span>)+</span><br><span class="line">  xlab(<span class="string">"年龄分段"</span>)+ylab(<span class="string">"本赛季总进球数"</span>)</span><br><span class="line"><span class="comment">#本赛季与下赛季进球数</span></span><br><span class="line">ggplot(mdata,aes(x=进球,y=下一年进球,colour=位置))+geom_point(size=<span class="number">2.5</span>)+theme_bw()+</span><br><span class="line">  theme(panel.grid.major=element_blank(),panel.grid.minor=element_blank())+</span><br><span class="line">  annotate(<span class="string">"text"</span>,x=<span class="number">23</span>,y=<span class="number">31</span>,label=<span class="string">"苏亚雷斯"</span>)+</span><br><span class="line">  annotate(<span class="string">"text"</span>,x=<span class="number">26</span>,y=<span class="number">12</span>,label=<span class="string">"范佩西"</span>)</span><br><span class="line"><span class="comment">#变量相关性分析</span></span><br><span class="line">mdata1&lt;-mdata[<span class="number">10</span>:<span class="number">33</span>]</span><br><span class="line">mdata2&lt;-mdata1[,-c(<span class="number">11</span>,<span class="number">23</span>)]</span><br><span class="line"><span class="keyword">library</span>(corrplot)</span><br><span class="line">M=cor(mdata2)</span><br><span class="line">corrplot(M,type=<span class="string">"lower"</span>,tl.pos=<span class="string">"b"</span>,tl.cex=<span class="number">0.7</span>,tl.col = <span class="string">"black"</span>)</span><br></pre></td></tr></table></figure><h2 id="四、模型建立"><a href="#四、模型建立" class="headerlink" title="四、模型建立"></a>四、模型建立</h2><h3 id="（一）建模思路"><a href="#（一）建模思路" class="headerlink" title="（一）建模思路"></a>（一）建模思路</h3><p>本案例考察22个影响因素对因变量下一个赛季进球数的影响，由于变量数目较多，部分变量间存在较强的相关关系，所以首先对变量降维。变量间存在某一些结构性特征，希望能寻求反映某方面信息的一组公共潜在因素。最终选择因子分析对数据首先进行降维。</p><h3 id="（二）因子分析"><a href="#（二）因子分析" class="headerlink" title="（二）因子分析"></a>（二）因子分析</h3><p>本案例首先对所有22个变量提取3个公因子，发现因子旋转后因子载荷矩阵中，变量红牌和乌龙球在公因子上的区分度较差，同时从相关系数矩阵也可以发现这两个变量与其他变量的相关性比较弱，与下一年进球数的相关系数分别为-0.001,0.033，影响程度很小，考虑删除这两个变量做分析。</p><p>对20个变量做因子分析，碎石图6显示，前四个主因子的特征根大于平行分析的结果，结合特质根大于1，选择三个主因子。经过因子旋转后，三个公因子特征根均大于1，合计可以解释60%的变差。载荷矩阵见表2，其中隐去了小值系数。</p><p><img src="/2019/09/02/%E8%8B%B1%E8%B6%85%E7%90%83%E5%91%98%E8%BF%9B%E7%90%83%E6%95%B0%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/image013.png" alt="png"></p><center>图6：公共因子碎石图</center><center> 表2：因子分析结果汇总表</center>![png](英超球员进球数影响因素分析/image0021.png)<p>由表2载荷矩阵可知，公因子1表示进攻因子，反映球员辅助进攻及射门技能；公因子2表示后卫因子，辅助解围维控全场格局争取有利优势；公因子3表示防守因子，与中场传球拦截相关。共同度大都在0.4以上，三个公因子对各个变量的解释程度较好。</p><p>根据因子得分分别找出在三个因子上得分最高的10名球员，如表3。其中，进攻力较强的大多是前锋，后卫力较强的都是后卫，防守力较强的大多是中场。与不同位置球员功能相关，符合认知。</p><center>表3：各因子表现较好的球员</center>![png](英超球员进球数影响因素分析/image0023.png)<h3 id="（三）回归分析"><a href="#（三）回归分析" class="headerlink" title="（三）回归分析"></a>（三）回归分析</h3><p>本案例中要预测的因变量是下一赛季进球数，球员的进球数可以认为服从泊松分布。建立泊松回归模型：</p><center>log(下一年进球数)~位置+年龄+进攻因子+后卫因子+防守因子</center>模型中使用的自变量有球员位置、球员年龄分段、上述三个因子。模型的偏差为489.18，空模型的偏差为901.07，可以认为模型是显著的。其中，前锋位置相比后卫可以提升进球数，中场提升幅度相比较小。球员的年龄段在25-30之间能力最强，球场上贡献的进球数最多。另外三个主因子中，进攻因子和防守因子对进球数都有正的影响，进球更多地会发生在球员进攻射门、传球带球的场合下，相比而言后卫的功能更多是防守解围，进球机会相对比较小。<center>表4：回归系数结果表</center>![png](英超球员进球数影响因素分析/image004.png)<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##因子分析</span></span><br><span class="line"><span class="comment">#因子数量的选择</span></span><br><span class="line"><span class="keyword">library</span>(psych)</span><br><span class="line">fa.parallel(scale(mdata2),fa=<span class="string">"fa"</span>,main=<span class="string">"碎石图"</span>)</span><br><span class="line"><span class="comment">#估计因子载荷</span></span><br><span class="line">fa1=fa(mdata2,nfactors=<span class="number">3</span>,rotate=<span class="string">'none'</span>,fm=<span class="string">'pa'</span>);fa1</span><br><span class="line">fa2=fa(mdata2,nfactors=<span class="number">3</span>,rotate=<span class="string">"varimax"</span>,fm=<span class="string">"pa"</span>);fa2 <span class="comment">#0.54</span></span><br><span class="line">mdata3&lt;-mdata2[,-c(<span class="number">7</span>,<span class="number">22</span>)]</span><br><span class="line">dim(mdata3)</span><br><span class="line">fa.parallel(scale(mdata3),fa=<span class="string">"fa"</span>,main=<span class="string">"碎石图"</span>)</span><br><span class="line">fa3=fa(mdata3,nfactors=<span class="number">3</span>,rotate=<span class="string">"varimax"</span>,fm=<span class="string">"pa"</span>);fa3</span><br><span class="line"><span class="comment">#计算因子得分</span></span><br><span class="line">mdata$scores1&lt;-fa3$scores[,<span class="number">1</span>]<span class="comment">#公因子1得分</span></span><br><span class="line">mdata$scores2&lt;-fa3$scores[,<span class="number">2</span>]<span class="comment">#公因子2得分</span></span><br><span class="line">mdata$scores3&lt;-fa3$scores[,<span class="number">3</span>]<span class="comment">#公因子3得分</span></span><br><span class="line">head(mdata[order(mdata$scores1,decreasing = <span class="literal">T</span>),c(<span class="number">1</span>,<span class="number">5</span>)],<span class="number">10</span>)<span class="comment">##进攻能力前6</span></span><br><span class="line">head(mdata[order(mdata$scores2,decreasing = <span class="literal">T</span>),c(<span class="number">1</span>,<span class="number">5</span>)],<span class="number">10</span>)<span class="comment">##解围能力前6</span></span><br><span class="line">head(mdata[order(mdata$scores3,decreasing = <span class="literal">T</span>),c(<span class="number">1</span>,<span class="number">5</span>)],<span class="number">10</span>)<span class="comment">##防守能力前6</span></span><br><span class="line"><span class="comment">#回归分析</span></span><br><span class="line">fit &lt;- glm(下一年进球~位置+年龄分段+scores1+scores2+scores3, family=poisson(link=log), data=mdata)</span><br><span class="line">summary(fit)</span><br><span class="line">step(fit)</span><br><span class="line">anova(glm(下一年进球~<span class="number">1</span>, family=poisson(link=log), data=mdata),fit)</span><br></pre></td></tr></table></figure><h2 id="五、结论"><a href="#五、结论" class="headerlink" title="五、结论"></a>五、结论</h2><p>本案例分析了英超联盟16只球队166名球员在2012-2013的表现情况，并选择合适的变量对下一赛季的进球数做预测。可以发现，球员位置和球员年龄段是影响进球数的两个自身因素。一般地，处于前锋位置的球员，承担进攻和射门任务，有更多进球机会，进球均值高；年龄段在25-30之间的球员，拥有一定的比赛经验，自身能力和身体素质处于最优状态，更容易在球场上进球。关于球员在球场上的表现，本案例使用因子分析，从20个因素中选取3个主因子，分别表示进攻因子，后卫因子，防守因子，三个因子对进球数都有显著的影响，其中进攻力和防守力呈正相关，后卫力更多地与球员的防守解围相关联，进球机会较少，相比对进球数有负的影响。</p><blockquote><p><strong>获取数据可留言笔者</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
        
    
    </summary>
    
      <category term="结构化数据" scheme="http://Raina-fighting.github.io/categories/%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="R" scheme="http://Raina-fighting.github.io/tags/R/"/>
    
      <category term="可视化" scheme="http://Raina-fighting.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="回归分析" scheme="http://Raina-fighting.github.io/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="因子分析" scheme="http://Raina-fighting.github.io/tags/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据流聚类算法比较并用于文本主题挖掘</title>
    <link href="http://Raina-fighting.github.io/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/"/>
    <id>http://Raina-fighting.github.io/2019/09/01/数据流聚类算法比较并用于文本主题挖掘/</id>
    <published>2019-09-01T10:51:46.000Z</published>
    <updated>2019-09-09T14:58:11.958Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">  数据流聚类算法比较并用于文本主题挖掘  </h1>## 分析提要<p>数据流的聚类要求能实时计算，对聚类变化进行跟踪。Birch 算法是数据流中一个重要的层次聚类法，能够用一遍扫描有效地进行聚类。</p><ul><li><p>通过模拟数据，对比 Birch 和 MiniBatchKMeans 算法，后者应用广泛，分批处理的技巧在大规模数据中很适用。模拟结果发现，二者的聚类精度相当，都能有效提高运算速度，在模拟数据上聚类效果都很好，且大批量数据集上 MiniBatchKMeans 聚类较快。</p></li><li><p>进行实证分析，使用 YouTube 网站每日热门视频榜单<a href="https://www.kaggle.com/datasnaek/youtube-new/data" target="_blank" rel="noopener" title="通过YouTube API获取">1</a>，特别关注娱乐类视频的标题内容，找到其中用户关心的主题。Birch 算法实现对标题文本向量化矩阵的聚类，选择提取了 20 个事件类，结合 lda挖掘的主题，可以有很好的解释。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''加载模块'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.colors <span class="keyword">as</span> colors</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans, KMeans, Birch</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> v_measure_score, homogeneity_score, completeness_score</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> Birch</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> LatentDirichletAllocation</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer,CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> TruncatedSVD</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">from</span> textblob <span class="keyword">import</span> TextBlob</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> RegexpTokenizer</span><br><span class="line"><span class="keyword">from</span> stop_words <span class="keyword">import</span> get_stop_words <span class="comment">#!对比nltk的停词表</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> sent_tokenize, word_tokenize</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, STOPWORDS</span><br><span class="line"><span class="keyword">import</span> pyLDAvis</span><br><span class="line"><span class="keyword">import</span> pyLDAvis.sklearn</span><br><span class="line"><span class="keyword">import</span> heapq</span><br></pre></td></tr></table></figure><h2 id="一、模拟数据"><a href="#一、模拟数据" class="headerlink" title="一、模拟数据"></a>一、模拟数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 生成高斯分布数据</span></span><br><span class="line">np.random.seed(<span class="number">2020</span>)</span><br><span class="line">num = <span class="number">200</span></span><br><span class="line"><span class="comment"># 标准圆形</span></span><br><span class="line">mean = [<span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">cov = [[<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>,<span class="number">1</span>]] </span><br><span class="line">x1,y1 = np.random.multivariate_normal(mean,cov,num).T</span><br><span class="line">plt.plot(x1,y1,<span class="string">'o'</span>,color=<span class="string">'green'</span>,alpha=<span class="number">0.35</span>)</span><br><span class="line"><span class="comment"># 椭圆，椭圆的轴向与坐标平行</span></span><br><span class="line">mean = [<span class="number">2</span>,<span class="number">10</span>]</span><br><span class="line">cov = [[<span class="number">0.5</span>,<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>,<span class="number">3</span>]] </span><br><span class="line">x2,y2 = np.random.multivariate_normal(mean,cov,num).T</span><br><span class="line">plt.plot(x2,y2,<span class="string">'o'</span>,color=<span class="string">'red'</span>,alpha=<span class="number">0.35</span>)</span><br><span class="line"><span class="comment"># 椭圆，但是椭圆的轴与坐标轴不一定平行</span></span><br><span class="line">mean = [<span class="number">5</span>,<span class="number">5</span>]</span><br><span class="line">cov = [[<span class="number">1</span>,<span class="number">2.3</span>],</span><br><span class="line">       [<span class="number">2.3</span>,<span class="number">1.4</span>]] </span><br><span class="line">x3,y3 = np.random.multivariate_normal(mean,cov,num).T</span><br><span class="line">plt.plot(x3,y3,<span class="string">'o'</span>,color=<span class="string">'blue'</span>,alpha=<span class="number">0.35</span>)</span><br><span class="line">X = np.concatenate((x1,x2,x3)).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">Y = np.concatenate((y1,y2,y3)).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">data = np.hstack((X, Y))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_3_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">label=np.concatenate((np.ones(<span class="number">200</span>)*<span class="number">1</span>,np.ones(<span class="number">200</span>)*<span class="number">2</span>,np.ones(<span class="number">200</span>)*<span class="number">3</span>)).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">data = np.hstack((data,label))</span><br><span class="line">data.shape</span><br><span class="line">datapre=pd.DataFrame(data,columns=[<span class="string">'x'</span>,<span class="string">'y'</span>,<span class="string">'label'</span>])</span><br><span class="line">data_withlabel=datapre[[<span class="string">'x'</span>,<span class="string">'y'</span>]]</span><br></pre></td></tr></table></figure><h3 id="（一）MiniBatchKMeans"><a href="#（一）MiniBatchKMeans" class="headerlink" title="（一）MiniBatchKMeans"></a>（一）MiniBatchKMeans</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">rango_clusters = range(<span class="number">2</span>,<span class="number">7</span>)</span><br><span class="line">error_clusters = []</span><br><span class="line">sil_scores = []</span><br><span class="line"><span class="keyword">for</span> num_clusters <span class="keyword">in</span> rango_clusters:</span><br><span class="line">    clusters = MiniBatchKMeans(num_clusters)</span><br><span class="line">    clusters.fit(data_withlabel)</span><br><span class="line">    error_clusters.append(clusters.inertia_)</span><br><span class="line">    sil_score = silhouette_score(data_withlabel,clusters.labels_)</span><br><span class="line">    sil_scores.append(sil_score) <span class="comment">#从2类开始计算</span></span><br><span class="line">clusters_df = pd.DataFrame(&#123;<span class="string">"num_clusters"</span>:rango_clusters,<span class="string">"error_clusters"</span>:error_clusters&#125;)</span><br><span class="line">plt.plot(clusters_df.num_clusters,clusters_df.error_clusters,marker =<span class="string">"o"</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Kmeans Clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Error'</span>)</span><br><span class="line">plt.show()</span><br><span class="line">clusters_sil = pd.DataFrame(&#123;<span class="string">"num_clusters"</span>:rango_clusters,<span class="string">"sil_clusters"</span>:sil_scores&#125;)</span><br><span class="line">plt.plot(clusters_sil.num_clusters,clusters_sil.sil_clusters,marker =<span class="string">"o"</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Kmeans Clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Silhouette'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_6_0.png" alt="png"></p><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_6_1.png" alt="png"></p><blockquote><p><strong>外部度量–两种比较聚类效果的指标</strong></p></blockquote><ul><li>轮廓系数越大越好；baseline 0.5</li><li>组内误差越小越好</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''选择类别数为3'''</span></span><br><span class="line">t=time()</span><br><span class="line">kmeans1 = MiniBatchKMeans(n_clusters=<span class="number">3</span>, init=<span class="string">'k-means++'</span>, n_init=<span class="number">1</span>, max_iter=<span class="number">100</span>, tol=<span class="number">0.0</span>,verbose=<span class="number">0</span>, random_state=<span class="literal">None</span>, batch_size=<span class="number">100</span>)</span><br><span class="line">kY = kmeans1.fit_predict(data_withlabel)</span><br><span class="line">time_=time()-t</span><br><span class="line">print(time_)</span><br></pre></td></tr></table></figure><pre><code>0.021027326583862305</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kmeans1.labels_[kmeans1.labels_==<span class="number">1</span>]=<span class="number">3</span></span><br><span class="line">kmeans1.labels_[kmeans1.labels_==<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">f, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="literal">True</span>, figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">ax1.scatter(datapre.x,datapre.y,  c=kY, cmap = <span class="string">"jet"</span>, edgecolor = <span class="string">"None"</span>, alpha=<span class="number">0.35</span>)</span><br><span class="line">ax1.set_title(<span class="string">'Clusters por k - means'</span>)</span><br><span class="line">ax2.scatter(datapre.x,datapre.y,  c=datapre.label, cmap = <span class="string">"jet"</span>, edgecolor = <span class="string">"None"</span>, alpha=<span class="number">0.35</span>)</span><br><span class="line">ax2.set_title(<span class="string">'Clusters Actuales'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_9_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''输出混淆矩阵和聚类中心'''</span></span><br><span class="line">print(confusion_matrix(datapre.label,kmeans1.labels_))</span><br><span class="line">print(classification_report(datapre.label,kmeans1.labels_))</span><br><span class="line">kmeans1.cluster_centers_</span><br></pre></td></tr></table></figure><pre><code>[[200   0   0] [  0 190  10] [ 11   0 189]]             precision    recall  f1-score   support        1.0       0.95      1.00      0.97       200        2.0       1.00      0.95      0.97       200        3.0       0.95      0.94      0.95       200avg / total       0.97      0.96      0.96       600</code></pre><p>​    </p><pre><code>array([[ 9.59338454,  9.7092478 ],       [ 1.94123116, 10.39578717],       [ 4.59653775,  5.07769715]])</code></pre><blockquote><p><strong>聚类标签与原始标签对比</strong></p></blockquote><ul><li>准确率为0.98，600个样本13个分错</li><li>第一类的召回率最高，达到100%</li><li>第二类与第三类有少数预测错误的</li></ul><h3 id="（二）Bitch"><a href="#（二）Bitch" class="headerlink" title="（二）Bitch"></a>（二）Bitch</h3><P>一种无监督的数据挖掘算法，用于创建特别是在非常大的数据集上的分层集群。一种有效的内存算法，可以在线学习，作为MiniBatchKMeans的替代方案。</P><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">num_clusters = range(<span class="number">2</span>,<span class="number">7</span>)</span><br><span class="line">v_measure = []</span><br><span class="line">b_measure = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> num_clusters:</span><br><span class="line">    kmeans_model = MiniBatchKMeans(n_clusters=num,init=<span class="string">'k-means++'</span>, n_init=<span class="number">1</span>, max_iter=<span class="number">100</span>, tol=<span class="number">0.0</span>, verbose=<span class="number">0</span>, random_state=<span class="literal">None</span>, batch_size=<span class="number">100</span>)</span><br><span class="line">    kmeans = kmeans_model.fit(data_withlabel)</span><br><span class="line">    kmeans_clusters = kmeans.predict(data_withlabel)</span><br><span class="line">    <span class="comment">#kmeans_distances = kmeans.transform(vec_matrix)</span></span><br><span class="line">    </span><br><span class="line">    b_model = Birch(n_clusters=int(num))</span><br><span class="line">    b = b_model.fit(data_withlabel)</span><br><span class="line">    b_clusters = b.predict(data_withlabel)</span><br><span class="line"></span><br><span class="line">    v_measure.append(v_measure_score(datapre.label, kmeans_clusters))</span><br><span class="line">    b_measure.append(v_measure_score(datapre.label, b_clusters))</span><br><span class="line">    </span><br><span class="line">fig = plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">plt.axvline(<span class="number">3</span>, color=<span class="string">'blue'</span>, linestyle=<span class="string">"dashed"</span>)</span><br><span class="line">plt.axvline(<span class="number">3</span>, color=<span class="string">'red'</span>, linestyle=<span class="string">"dashed"</span>)</span><br><span class="line">plt.title(<span class="string">'Choosing the optimal number of Clusters'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Number of Clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'V-measure'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.plot(num_clusters, v_measure, label=<span class="string">"K-Means"</span>, color=<span class="string">"blue"</span>)</span><br><span class="line">plt.plot(num_clusters, b_measure, label=<span class="string">"BIRCH"</span>, color=<span class="string">"red"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_14_1.png" alt="png"></p><blockquote><p><strong>已知类别的内部度量</strong></p></blockquote><ul><li>v = (1 + beta) * (homogeneity * completeness) / (beta * homogeneity + completeness)</li><li>homogeneity同质性度量；每一个聚出的类仅包含一个类别的程度度量。如果聚类结果的所有聚类仅包含属于单个类的数据点，则聚类结果满足同质性。这个度量独立于标签的绝对值:类别或聚类标签值的排列不会以任何方式改变分值。</li><li>completeness完备性度量；给定基本事实的聚类标记的完备性度量。如果属于给定类的所有数据点都是同一聚类的元素，则聚类结果满足完整性。这个度量独立于标签的绝对值:类别或聚类标签值的排列不会以任何方式改变分值。<br><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rango_clusters = range(<span class="number">2</span>,<span class="number">7</span>)</span><br><span class="line">sil_scores = []</span><br><span class="line"><span class="keyword">for</span> num_clusters <span class="keyword">in</span> rango_clusters:</span><br><span class="line">    clusters = Birch(n_clusters=num_clusters)</span><br><span class="line">    clusters.fit(data_withlabel)</span><br><span class="line">    sil_score = silhouette_score(data_withlabel,clusters.labels_)</span><br><span class="line">    sil_scores.append(sil_score) <span class="comment">#从2类开始计算</span></span><br><span class="line">    </span><br><span class="line">clusters_sil = pd.DataFrame(&#123;<span class="string">"num_clusters"</span>:rango_clusters,<span class="string">"sil_clusters"</span>:sil_scores&#125;)</span><br><span class="line">plt.plot(clusters_sil.num_clusters,clusters_sil.sil_clusters,marker =<span class="string">"o"</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Birch Clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Silhouette'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_16_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">birch1 = Birch(n_clusters=<span class="number">3</span>)</span><br><span class="line">bY = birch1.fit_predict(data_withlabel)</span><br><span class="line">bY[bY==<span class="number">0</span>]=<span class="number">3</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="literal">True</span>, figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">ax1.scatter(datapre.x,datapre.y,  c=bY, cmap = <span class="string">"jet"</span>, edgecolor = <span class="string">"None"</span>, alpha=<span class="number">0.35</span>)</span><br><span class="line">ax1.set_title(<span class="string">'Clusters por birch'</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(datapre.x,datapre.y,  c=datapre.label, cmap = <span class="string">"jet"</span>, edgecolor = <span class="string">"None"</span>, alpha=<span class="number">0.35</span>)</span><br><span class="line">ax2.set_title(<span class="string">'Clusters Actuales'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_18_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''输出混淆矩阵'''</span></span><br><span class="line">print(confusion_matrix(datapre.label,bY))</span><br><span class="line">print(classification_report(datapre.label,bY))</span><br></pre></td></tr></table></figure><pre><code>[[200   0   0] [  0 200   0] [ 20   4 176]]             precision    recall  f1-score   support        1.0       0.91      1.00      0.95       200        2.0       0.98      1.00      0.99       200        3.0       1.00      0.88      0.94       200avg / total       0.96      0.96      0.96       600</code></pre><p>​    </p><blockquote><p><strong>聚类标签与原始标签对比</strong></p></blockquote><ul><li>准确率为0.97，600个样本19个分错</li><li>第一类和第二类召回率均达到100%</li><li>第二类与第三类有少数预测错误的</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''调整参数'''</span></span><br><span class="line">birch2 = Birch(n_clusters=<span class="number">3</span>, branching_factor=<span class="number">10</span>, threshold=<span class="number">0.5</span>)</span><br><span class="line">bY = birch2.fit_predict(data_withlabel)</span><br><span class="line">bY[bY==<span class="number">0</span>]=<span class="number">3</span></span><br><span class="line">print(confusion_matrix(datapre.label, bY))</span><br><span class="line">print(classification_report(datapre.label, bY))</span><br></pre></td></tr></table></figure><pre><code>[[200   0   0] [  0 191   9] [  5   0 195]]             precision    recall  f1-score   support        1.0       0.98      1.00      0.99       200        2.0       1.00      0.95      0.98       200        3.0       0.96      0.97      0.97       200avg / total       0.98      0.98      0.98       600</code></pre><p>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''调整参数，不规定类别数'''</span></span><br><span class="line">t=time()</span><br><span class="line">birch3= Birch(branching_factor=<span class="number">200</span>, threshold=<span class="number">3</span>,n_clusters=<span class="literal">None</span>)</span><br><span class="line">bY = birch3.fit_predict(data_withlabel)</span><br><span class="line">time()-t</span><br></pre></td></tr></table></figure><pre><code>0.02101612091064453</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bY=bY+<span class="number">1</span></span><br><span class="line">print(confusion_matrix(datapre.label, bY))</span><br><span class="line">print(classification_report(datapre.label, bY))</span><br><span class="line">np.unique(birch3.labels_).size</span><br><span class="line">birch3.subcluster_centers_</span><br></pre></td></tr></table></figure><pre><code>[[200   0   0] [  0 195   5] [ 13   1 186]]             precision    recall  f1-score   support        1.0       0.94      1.00      0.97       200        2.0       0.99      0.97      0.98       200        3.0       0.97      0.93      0.95       200avg / total       0.97      0.97      0.97       600</code></pre><p>​    </p><pre><code>array([[8.86247657, 9.87152635],       [1.97577796, 9.90506347],       [4.93940824, 4.86512982]])</code></pre><p>注意参数：每个CF最大样本半径阈值；枝节点最大CF数</p>https://scikit-learn.org/stable/auto_examples/cluster/plot_birch_vs_minibatchkmeans.html#sphx-glr-auto-examples-cluster-plot-birch-vs-minibatchkmeans-py<h3 id="（三）多批次大批量数据聚类"><a href="#（三）多批次大批量数据聚类" class="headerlink" title="（三）多批次大批量数据聚类"></a>（三）多批次大批量数据聚类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 生成高斯分布数据</span></span><br><span class="line">np.random.seed(<span class="number">2020</span>)</span><br><span class="line">num = <span class="number">10000</span></span><br><span class="line"><span class="comment"># 标准圆形</span></span><br><span class="line">mean = [<span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">cov = [[<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>,<span class="number">1</span>]] </span><br><span class="line">x1,y1 = np.random.multivariate_normal(mean,cov,num).T</span><br><span class="line"><span class="comment"># 椭圆，椭圆的轴向与坐标平行</span></span><br><span class="line">mean = [<span class="number">2</span>,<span class="number">10</span>]</span><br><span class="line">cov = [[<span class="number">0.5</span>,<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>,<span class="number">3</span>]] </span><br><span class="line">x2,y2 = np.random.multivariate_normal(mean,cov,num).T</span><br><span class="line"><span class="comment"># 椭圆，但是椭圆的轴与坐标轴不一定平行</span></span><br><span class="line">mean = [<span class="number">5</span>,<span class="number">5</span>]</span><br><span class="line">cov = [[<span class="number">1</span>,<span class="number">2.3</span>],</span><br><span class="line">       [<span class="number">2.3</span>,<span class="number">1.4</span>]] </span><br><span class="line">x3,y3 = np.random.multivariate_normal(mean,cov,num).T</span><br><span class="line">X = np.concatenate((x1,x2,x3)).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">Y = np.concatenate((y1,y2,y3)).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">data = np.hstack((X, Y))</span><br><span class="line">label=np.concatenate((np.ones(num)*<span class="number">1</span>,np.ones(num)*<span class="number">2</span>,np.ones(num)*<span class="number">3</span>)).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">data = np.hstack((data,label))</span><br><span class="line">data.shape</span><br><span class="line">datapre=pd.DataFrame(data,columns=[<span class="string">'x'</span>,<span class="string">'y'</span>,<span class="string">'label'</span>])</span><br><span class="line">data_withlabel=datapre[[<span class="string">'x'</span>,<span class="string">'y'</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">datapre=datapre.sample(frac=<span class="number">1</span>)</span><br><span class="line">data_withlabel=datapre[[<span class="string">'x'</span>,<span class="string">'y'</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 选择类别数为3</span></span><br><span class="line">t=time()</span><br><span class="line">kmeans1 = MiniBatchKMeans(n_clusters=<span class="number">3</span>, init=<span class="string">'k-means++'</span>, n_init=<span class="number">1</span>, max_iter=<span class="number">100</span>, tol=<span class="number">0.0</span>, verbose=<span class="number">0</span>, random_state=<span class="literal">None</span>, batch_size=<span class="number">100</span>)</span><br><span class="line">kY = kmeans1.fit_predict(data_withlabel)</span><br><span class="line">time_=time()-t</span><br><span class="line">print(<span class="string">'kmeans'</span>,time_)</span><br><span class="line">t=time()</span><br><span class="line">birch2 = Birch(n_clusters=<span class="number">3</span>, branching_factor=<span class="number">200</span>, threshold=<span class="number">3</span>)</span><br><span class="line">bY = birch2.fit_predict(data_withlabel)</span><br><span class="line">time_=time()-t</span><br><span class="line">print(<span class="string">'birch'</span>,time_)</span><br></pre></td></tr></table></figure><pre><code>kmeans 0.12408709526062012birch 0.5316519737243652</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">k_score=v_measure_score(datapre.label, kY)</span><br><span class="line">b_score=v_measure_score(datapre.label, bY)</span><br><span class="line">print(<span class="string">"kmeans_score"</span>,k_score)</span><br><span class="line">print(<span class="string">"birch_score"</span>,b_score)</span><br></pre></td></tr></table></figure><pre><code>kmeans_score 0.8990013481586817birch_score 0.9012205641918879</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">k_sil=silhouette_score(data_withlabel,kY)</span><br><span class="line">b_sil=silhouette_score(data_withlabel,bY)</span><br></pre></td></tr></table></figure><h2 id="二、实证分析——-YouTube网站Entertainment类视频内容挖掘"><a href="#二、实证分析——-YouTube网站Entertainment类视频内容挖掘" class="headerlink" title="二、实证分析—— YouTube网站Entertainment类视频内容挖掘"></a>二、实证分析—— YouTube网站Entertainment类视频内容挖掘</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(<span class="string">"F:\\2019春学习\\期末报告\\文本-丰\\US_videos"</span>)</span><br><span class="line">df_usa = pd.read_csv(<span class="string">"USvideos.csv"</span>) <span class="comment">#(18973, 16)</span></span><br><span class="line">df_usa_multiple_day_trend= df_usa.drop_duplicates(subset=<span class="string">'video_id'</span>,keep=<span class="string">'first'</span>,inplace=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">'热门视频榜单保留第一次出现'</span>,df_usa_multiple_day_trend.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>热门视频榜单保留第一次出现 4079</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bloblist_title=[]</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df_usa_multiple_day_trend[<span class="string">'title'</span>]:</span><br><span class="line">    blob = TextBlob(row)</span><br><span class="line">    bloblist_title.append((row,blob.sentiment.polarity, blob.sentiment.subjectivity))</span><br><span class="line">    df_usa_polarity_title = pd.DataFrame(bloblist_title, columns = [<span class="string">'sentence'</span>,<span class="string">'sentiment'</span>,<span class="string">'polarity'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_title</span><span class="params">(df_usa_polarity_title)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> df_usa_polarity_title[<span class="string">'sentiment'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">        val = <span class="string">"Positive"</span></span><br><span class="line">    <span class="keyword">elif</span> df_usa_polarity_title[<span class="string">'sentiment'</span>] == <span class="number">0</span>:</span><br><span class="line">        val = <span class="string">"Neutral"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        val = <span class="string">"Negative"</span></span><br><span class="line">    <span class="keyword">return</span> val</span><br><span class="line">df_usa_polarity_title[<span class="string">'Sentiment_Type'</span>] = df_usa_polarity_title.apply(f_title, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">id_to_category = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"US_category_id.json"</span>,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    id_data = json.load(f)</span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> id_data[<span class="string">"items"</span>]:</span><br><span class="line">        id_to_category[category[<span class="string">"id"</span>]] = category[<span class="string">"snippet"</span>][<span class="string">"title"</span>]</span><br><span class="line">df_usa_multiple_day_trend[<span class="string">"category_id"</span>] = df_usa_multiple_day_trend[<span class="string">"category_id"</span>].astype(str)</span><br><span class="line">df_usa_multiple_day_trend.insert(<span class="number">4</span>, <span class="string">"category"</span>,df_usa_multiple_day_trend[<span class="string">"category_id"</span>].map(id_to_category))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df_usa_polarity_title[<span class="string">'category'</span>]=list(df_usa_multiple_day_trend[<span class="string">'category'</span>])</span><br><span class="line">df_usa_category_polar=df_usa_polarity_title[[<span class="string">'category'</span>,<span class="string">'Sentiment_Type'</span>,<span class="string">'sentiment'</span>]].groupby([<span class="string">'category'</span>,<span class="string">'Sentiment_Type'</span>]).count()</span><br><span class="line">a=df_usa_category_polar.ix[[<span class="string">'Entertainment'</span>]]</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax = sns.barplot(x=a.index.levels[<span class="number">1</span>],y=<span class="string">"sentiment"</span>, data=a)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.title(<span class="string">'Entertainment'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_36_0.png" alt="png" style="zoom:70%;"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">entertainment_title=df_usa_multiple_day_trend[<span class="string">'title'</span>].loc[df_usa_multiple_day_trend[<span class="string">'category'</span>]==<span class="string">'Entertainment'</span>]</span><br><span class="line">a = entertainment_title.str.lower().str.cat(sep=<span class="string">' '</span>) <span class="comment">#转换为空格连接的文本串</span></span><br><span class="line">b = re.sub(<span class="string">'[^A-Za-z]+'</span>, <span class="string">' '</span>, a) <span class="comment">#只保留了英文字母--^从开头匹配；+号多个匹配</span></span><br><span class="line">stop_words = list(get_stop_words(<span class="string">'en'</span>))         </span><br><span class="line">nltk_words = [<span class="string">'i'</span>, <span class="string">'me'</span>, <span class="string">'my'</span>, <span class="string">'myself'</span>, <span class="string">'we'</span>, <span class="string">'our'</span>, <span class="string">'ours'</span>, <span class="string">'ourselves'</span>, <span class="string">'you'</span>, <span class="string">'your'</span>, <span class="string">'yours'</span>, <span class="string">'yourself'</span>, <span class="string">'yourselves'</span>, <span class="string">'he'</span>, <span class="string">'him'</span>, <span class="string">'his'</span>, <span class="string">'himself'</span>, <span class="string">'she'</span>, <span class="string">'her'</span>, <span class="string">'hers'</span>, <span class="string">'herself'</span>, <span class="string">'it'</span>, <span class="string">'its'</span>, <span class="string">'itself'</span>, <span class="string">'they'</span>, <span class="string">'them'</span>, <span class="string">'their'</span>, <span class="string">'theirs'</span>, <span class="string">'themselves'</span>, <span class="string">'what'</span>, <span class="string">'which'</span>, <span class="string">'who'</span>, <span class="string">'whom'</span>, <span class="string">'this'</span>, <span class="string">'that'</span>, <span class="string">'these'</span>, <span class="string">'those'</span>, <span class="string">'am'</span>, <span class="string">'is'</span>, <span class="string">'are'</span>, <span class="string">'was'</span>, <span class="string">'were'</span>, <span class="string">'be'</span>, <span class="string">'been'</span>, <span class="string">'being'</span>, <span class="string">'have'</span>, <span class="string">'has'</span>, <span class="string">'had'</span>, <span class="string">'having'</span>, <span class="string">'do'</span>, <span class="string">'does'</span>, <span class="string">'did'</span>, <span class="string">'doing'</span>, <span class="string">'a'</span>, <span class="string">'an'</span>, <span class="string">'the'</span>, <span class="string">'and'</span>, <span class="string">'but'</span>, <span class="string">'if'</span>, <span class="string">'or'</span>, <span class="string">'because'</span>, <span class="string">'as'</span>, <span class="string">'until'</span>, <span class="string">'while'</span>, <span class="string">'of'</span>, <span class="string">'at'</span>, <span class="string">'by'</span>, <span class="string">'for'</span>, <span class="string">'with'</span>, <span class="string">'about'</span>, <span class="string">'against'</span>, <span class="string">'between'</span>, <span class="string">'into'</span>, <span class="string">'through'</span>, <span class="string">'during'</span>, <span class="string">'before'</span>, <span class="string">'after'</span>, <span class="string">'above'</span>, <span class="string">'below'</span>, <span class="string">'to'</span>, <span class="string">'from'</span>, <span class="string">'up'</span>, <span class="string">'down'</span>, <span class="string">'in'</span>, <span class="string">'out'</span>, <span class="string">'on'</span>, <span class="string">'off'</span>, <span class="string">'over'</span>, <span class="string">'under'</span>, <span class="string">'again'</span>, <span class="string">'further'</span>, <span class="string">'then'</span>, <span class="string">'once'</span>, <span class="string">'here'</span>, <span class="string">'there'</span>, <span class="string">'when'</span>, <span class="string">'where'</span>, <span class="string">'why'</span>, <span class="string">'how'</span>, <span class="string">'all'</span>, <span class="string">'any'</span>, <span class="string">'both'</span>, <span class="string">'each'</span>, <span class="string">'few'</span>, <span class="string">'more'</span>, <span class="string">'most'</span>, <span class="string">'other'</span>, <span class="string">'some'</span>, <span class="string">'such'</span>, <span class="string">'no'</span>, <span class="string">'nor'</span>, <span class="string">'not'</span>, <span class="string">'only'</span>, <span class="string">'own'</span>, <span class="string">'same'</span>, <span class="string">'so'</span>, <span class="string">'than'</span>, <span class="string">'too'</span>, <span class="string">'very'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'can'</span>, <span class="string">'will'</span>, <span class="string">'just'</span>, <span class="string">'don'</span>, <span class="string">'should'</span>, <span class="string">'now'</span>]</span><br><span class="line"><span class="comment">#nltk.download("stopwords") #！连接失败 http://www.nltk.org/nltk_data/ http://johnlaudun.org/20130126-nltk-stopwords/</span></span><br><span class="line">stop_words.extend(nltk_words)</span><br><span class="line">word_tokens = word_tokenize(b) <span class="comment">#nltk分词!tokenize</span></span><br><span class="line">filtered_word = [w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokens <span class="keyword">if</span> <span class="keyword">not</span> w <span class="keyword">in</span> stop_words] </span><br><span class="line">print(<span class="string">"nltk分词后词语个数"</span>,len(filtered_word))</span><br><span class="line"><span class="comment">## 去掉长度小于等于2词语</span></span><br><span class="line">processed_word = [word <span class="keyword">for</span> word <span class="keyword">in</span> filtered_word <span class="keyword">if</span> len(word) &gt; <span class="number">2</span>]</span><br><span class="line">print(<span class="string">"处理过后词语个数"</span>,len(processed_word))</span><br><span class="line">word_dist = nltk.FreqDist(processed_word)</span><br></pre></td></tr></table></figure><pre><code>nltk分词后词语个数 6596处理过后词语个数 6248</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wc</span><span class="params">(data,bgcolor,title)</span>:</span></span><br><span class="line">    plt.figure(figsize = (<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">    wc = WordCloud(background_color = bgcolor, max_words = <span class="number">1000</span>,  max_font_size = <span class="number">50</span>)</span><br><span class="line">    wc.generate(<span class="string">' '</span>.join(data))</span><br><span class="line">    plt.imshow(wc)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">wc(processed_word,<span class="string">'black'</span>,<span class="string">'Common Words'</span> )</span><br></pre></td></tr></table></figure><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_40_0.png" alt="png" style="zoom:70%;"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''动态lda结果展示'''</span></span><br><span class="line">vectorizer_entertainment_title = CountVectorizer(min_df=<span class="number">5</span>, max_df=<span class="number">0.9</span>, stop_words=<span class="string">'english'</span>, lowercase=<span class="literal">True</span>, token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line">entertainment_title_vectorized = vectorizer_entertainment_title.fit_transform(entertainment_title)</span><br><span class="line">lda_popular_entertainment_title_vectorized = LatentDirichletAllocation(n_topics=<span class="number">7</span>, max_iter=<span class="number">5</span>, learning_method=<span class="string">'online'</span>)</span><br><span class="line">entertainment_title_vectorized_lda = lda_popular_entertainment_title_vectorized.fit_transform(entertainment_title_vectorized )</span><br><span class="line">dash = pyLDAvis.sklearn.prepare(lda_popular_entertainment_title_vectorized,entertainment_title_vectorized, vectorizer_entertainment_title, mds=<span class="string">'tsne'</span>)</span><br><span class="line">pyLDAvis.show(dash)</span><br></pre></td></tr></table></figure><h3 id="Birch视频标题内容聚类"><a href="#Birch视频标题内容聚类" class="headerlink" title="Birch视频标题内容聚类"></a>Birch视频标题内容聚类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vectorizer = TfidfVectorizer(min_df=<span class="number">0</span>, analyzer=<span class="string">'word'</span>, max_features=<span class="number">1000</span>, ngram_range=(<span class="number">1</span>,<span class="number">2</span>),stop_words=<span class="string">'english'</span>,token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line">vec_matrix = vectorizer.fit_transform(entertainment_title)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">svd = TruncatedSVD(n_components=<span class="number">50</span>, random_state=<span class="number">0</span>)</span><br><span class="line">svd_tfidf = svd.fit_transform(vec_matrix)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tsne_model = TSNE(n_components=<span class="number">2</span>, verbose=<span class="number">1</span>, random_state=<span class="number">0</span>, n_iter=<span class="number">500</span>)</span><br><span class="line">tsne_tfidf = tsne_model.fit_transform(svd_tfidf)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tsne_tfidf_df = pd.DataFrame(tsne_tfidf)</span><br><span class="line">tsne_tfidf_df.columns = [<span class="string">'x'</span>, <span class="string">'y'</span>]</span><br><span class="line">tsne_tfidf_df[<span class="string">'title'</span>] = entertainment_title</span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">plt.margins(<span class="number">0.05</span>) <span class="comment"># Optional, just adds 5% padding to the autoscaling</span></span><br><span class="line">plt.plot(tsne_tfidf_df.x, tsne_tfidf_df.y, marker=<span class="string">'o'</span>, linestyle=<span class="string">''</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_47_0.png" alt="png" style="zoom:70%;"><blockquote><p>TSNE，是一种概率技术，它在较小维度的嵌入空间中维护数据的局部结构，它在二维平面中生成数据的“投影”。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''调节参数：类别数'''</span></span><br><span class="line">rango_clusters = range(<span class="number">2</span>,<span class="number">50</span>)</span><br><span class="line">sil_scores = []</span><br><span class="line"><span class="keyword">for</span> num_clusters <span class="keyword">in</span> rango_clusters:</span><br><span class="line">    clusters = Birch(n_clusters=int(num_clusters))</span><br><span class="line">    clusters.fit(vec_matrix)</span><br><span class="line">    sil_score = silhouette_score(vec_matrix,clusters.predict(vec_matrix))</span><br><span class="line">    sil_scores.append(sil_score) <span class="comment">#从2类开始计算</span></span><br><span class="line">    </span><br><span class="line">clusters_sil = pd.DataFrame(&#123;<span class="string">"num_clusters"</span>:rango_clusters,<span class="string">"sil_clusters"</span>:sil_scores&#125;)</span><br><span class="line">plt.plot(clusters_sil.num_clusters,clusters_sil.sil_clusters,marker =<span class="string">"o"</span>)</span><br><span class="line">plt.axvline(<span class="number">20</span>, color=<span class="string">'blue'</span>, linestyle=<span class="string">"dashed"</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Birch Clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Silhouette'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_49_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''调节参数：保留特征数'''</span></span><br><span class="line">num_clusters = <span class="number">20</span></span><br><span class="line">max_features = np.arange(<span class="number">400</span>,<span class="number">2000</span>,<span class="number">50</span>)</span><br><span class="line">sil_scores = []</span><br><span class="line"><span class="keyword">for</span> features <span class="keyword">in</span> max_features:</span><br><span class="line">    vectorizer = TfidfVectorizer(min_df=<span class="number">0</span>, analyzer=<span class="string">'word'</span>, max_features=features, ngram_range=(<span class="number">1</span>,<span class="number">2</span>),stop_words=<span class="string">'english'</span>,token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line">    vec_matrix = vectorizer.fit_transform(entertainment_title)</span><br><span class="line"></span><br><span class="line">    clusters = Birch(n_clusters=int(num_clusters))</span><br><span class="line">    clusters.fit(vec_matrix)</span><br><span class="line">    sil_score = silhouette_score(vec_matrix,clusters.predict(vec_matrix))</span><br><span class="line">    sil_scores.append(sil_score)</span><br><span class="line">    </span><br><span class="line">clusters_sil = pd.DataFrame(&#123;<span class="string">"num_clusters"</span>:max_features,<span class="string">"sil_clusters"</span>:sil_scores&#125;)</span><br><span class="line">plt.plot(clusters_sil.num_clusters,clusters_sil.sil_clusters,marker =<span class="string">"o"</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Birch Clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Silhouette'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_51_1.png" alt="png" style="zoom:100%;"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vectorizer = TfidfVectorizer(min_df=<span class="number">0</span>, analyzer=<span class="string">'word'</span>, ngram_range=(<span class="number">1</span>,<span class="number">1</span>),stop_words=<span class="string">'english'</span>) <span class="comment">#7292-2888</span></span><br><span class="line">vec_matrix = vectorizer.fit_transform(entertainment_title)</span><br><span class="line">len(vectorizer.get_feature_names())</span><br></pre></td></tr></table></figure><pre><code>2888</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_clusters = <span class="number">20</span></span><br><span class="line">features = <span class="number">500</span></span><br><span class="line">vectorizer = TfidfVectorizer(min_df=<span class="number">0</span>, max_features=features, analyzer=<span class="string">'word'</span>, ngram_range=(<span class="number">1</span>,<span class="number">2</span>), stop_words=<span class="string">'english'</span>)</span><br><span class="line">vec_matrix = vectorizer.fit_transform(entertainment_title)</span><br><span class="line">ms_model = Birch(n_clusters=num_clusters)</span><br><span class="line">ms = ms_model.fit(vec_matrix)</span><br><span class="line">ms_clusters = ms.predict(vec_matrix)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tsne_model = TSNE(n_components=<span class="number">2</span>, verbose=<span class="number">1</span>, random_state=<span class="number">0</span>, n_iter=<span class="number">500</span>)</span><br><span class="line">tsne_lda = tsne_model.fit_transform(ms.transform(vec_matrix))</span><br><span class="line">ms_df = pd.DataFrame(tsne_lda, columns=[<span class="string">'x'</span>, <span class="string">'y'</span>])</span><br><span class="line">ms_df[<span class="string">'cluster'</span>] = ms_clusters</span><br><span class="line">ms_df[<span class="string">'cluster'</span>] = ms_df[<span class="string">'cluster'</span>].map(str)</span><br><span class="line">ms_df[<span class="string">'title'</span>] = entertainment_title</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sns.set_palette(sns.color_palette(<span class="string">"hls"</span>, num_clusters))</span><br><span class="line">groups = ms_df.groupby(<span class="string">'cluster'</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">ax.margins(<span class="number">0.05</span>)</span><br><span class="line">cluster_news = []</span><br><span class="line"><span class="keyword">for</span> name, group <span class="keyword">in</span> groups:</span><br><span class="line">    min_dist = <span class="number">100</span></span><br><span class="line">    min_ind = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> group.index:</span><br><span class="line">        <span class="keyword">if</span> ms.transform(vec_matrix)[ind][int(name)] &lt; min_dist:</span><br><span class="line">            min_dist = ms.transform(vec_matrix)[ind][int(name)]</span><br><span class="line">            min_ind = ind</span><br><span class="line">            </span><br><span class="line">    cluster_news.append(entertainment_title.iloc[min_ind])</span><br><span class="line">    ax.plot(group.x, group.y, marker=<span class="string">'o'</span>, linestyle=<span class="string">''</span>, label=name)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_55_0.png" alt="png" style="zoom:67%;"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''每一类的含义和频数统计'''</span></span><br><span class="line">name=vectorizer.get_feature_names()</span><br><span class="line">result=pd.DataFrame(list(vec_matrix.A),columns=name)</span><br><span class="line">result[<span class="string">'category'</span>]=ms_clusters</span><br><span class="line">a=result.groupby([<span class="string">'category'</span>]).mean()</span><br><span class="line">num=[]</span><br><span class="line">title=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(a.shape[<span class="number">0</span>]):</span><br><span class="line">    num.append(sum(a.loc[i]!=<span class="number">0</span>))</span><br><span class="line">    aa=list(a.loc[i])</span><br><span class="line">    rank=list(map(aa.index, heapq.nlargest(<span class="number">20</span>, aa)))</span><br><span class="line">    title.append(<span class="string">' '</span>.join(a.columns[rank]))</span><br><span class="line">a[<span class="string">'not zero'</span>]=num</span><br><span class="line">a[<span class="string">'count'</span>]=list(Counter(ms_clusters).values())</span><br><span class="line">a[<span class="string">'title'</span>]=title</span><br></pre></td></tr></table></figure><h4 id="调节特征维度"><a href="#调节特征维度" class="headerlink" title="调节特征维度"></a>调节特征维度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vectorizer = TfidfVectorizer(analyzer=<span class="string">'word'</span>, ngram_range=(<span class="number">1</span>,<span class="number">1</span>), stop_words=<span class="string">'english'</span>)</span><br><span class="line">vec_matrix = vectorizer.fit_transform(entertainment_title)</span><br><span class="line">ms_model = Birch(n_clusters=num_clusters)</span><br><span class="line">ms = ms_model.fit(vec_matrix)</span><br><span class="line">ms_clusters = ms.predict(vec_matrix)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tsne_model = TSNE(n_components=<span class="number">2</span>, verbose=<span class="number">1</span>, random_state=<span class="number">0</span>, n_iter=<span class="number">500</span>)</span><br><span class="line">tsne_lda = tsne_model.fit_transform(ms.transform(vec_matrix))</span><br><span class="line">ms_df = pd.DataFrame(tsne_lda, columns=[<span class="string">'x'</span>, <span class="string">'y'</span>])</span><br><span class="line">ms_df[<span class="string">'cluster'</span>] = ms_clusters</span><br><span class="line">ms_df[<span class="string">'cluster'</span>] = ms_df[<span class="string">'cluster'</span>].map(str)</span><br><span class="line">ms_df[<span class="string">'title'</span>] = entertainment_title</span><br><span class="line">sns.set_palette(sns.color_palette(<span class="string">"hls"</span>, num_clusters))</span><br><span class="line">groups = ms_df.groupby(<span class="string">'cluster'</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line">ax.margins(<span class="number">0.05</span>) </span><br><span class="line"></span><br><span class="line">cluster_news = []</span><br><span class="line"><span class="keyword">for</span> name, group <span class="keyword">in</span> groups:</span><br><span class="line">    min_dist = <span class="number">100</span></span><br><span class="line">    min_ind = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> group.index:</span><br><span class="line">        <span class="keyword">if</span> ms.transform(vec_matrix)[ind][int(name)] &lt; min_dist:</span><br><span class="line">            min_dist = ms.transform(vec_matrix)[ind][int(name)]</span><br><span class="line">            min_ind = ind</span><br><span class="line">            </span><br><span class="line">    cluster_news.append(entertainment_title.iloc[min_ind])</span><br><span class="line">    ax.plot(group.x, group.y, marker=<span class="string">'o'</span>, linestyle=<span class="string">''</span>, label=name)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2019/09/01/%E6%95%B0%E6%8D%AE%E6%B5%81%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%E5%B9%B6%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%8C%96%E6%8E%98/output_63_1.png" alt="png" style="zoom:67%;"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">name=vectorizer.get_feature_names()</span><br><span class="line">result=pd.DataFrame(list(vec_matrix.A),columns=name)</span><br><span class="line">result[<span class="string">'category'</span>]=ms_clusters</span><br><span class="line">a=result.groupby([<span class="string">'category'</span>]).mean()</span><br><span class="line">num=[]</span><br><span class="line">title=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(a.shape[<span class="number">0</span>]):</span><br><span class="line">    num.append(sum(a.loc[i]!=<span class="number">0</span>))</span><br><span class="line">    aa=list(a.loc[i])</span><br><span class="line">    rank=list(map(aa.index, heapq.nlargest(<span class="number">10</span>, aa)))</span><br><span class="line">    title.append(<span class="string">' '</span>.join(a.columns[rank]))</span><br><span class="line">a[<span class="string">'not zero'</span>]=num</span><br><span class="line">a[<span class="string">'count'</span>]=list(ms_df[<span class="string">'cluster'</span>].value_counts().sort_index())</span><br><span class="line">a[<span class="string">'title'</span>]=title</span><br><span class="line">a[[<span class="string">'title'</span>,<span class="string">'count'</span>]]</span><br></pre></td></tr></table></figure><div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>title</th>      <th>count</th>    </tr>    <tr>      <th>category</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>movie justice justice talk review studios pant...</td>      <td>41</td>    </tr>    <tr>      <th>1</th>      <td>new challenge wwhl stars ellen diy day 2017 li...</td>      <td>589</td>    </tr>    <tr>      <th>2</th>      <td>2017 voice amazon youtubers people movies jenn...</td>      <td>4</td>    </tr>    <tr>      <th>3</th>      <td>christmas trump melania doctor state state tre...</td>      <td>5</td>    </tr>    <tr>      <th>4</th>      <td>2018 globes globes awards grammys sag moments ...</td>      <td>24</td>    </tr>    <tr>      <th>5</th>      <td>season hd tour kardashians keeping trailer gra...</td>      <td>13</td>    </tr>    <tr>      <th>6</th>      <td>graham graham dornan dornan perfect mic drop p...</td>      <td>22</td>    </tr>    <tr>      <th>7</th>      <td>official trailer hd netflix mirror black seaso...</td>      <td>20</td>    </tr>    <tr>      <th>8</th>      <td>things searched searched wired stranger questi...</td>      <td>7</td>    </tr>    <tr>      <th>9</th>      <td>trailer official teaser hd world ready ocean p...</td>      <td>9</td>    </tr>    <tr>      <th>10</th>      <td>fear fear weird stuff box box box bearded cast...</td>      <td>13</td>    </tr>    <tr>      <th>11</th>      <td>logan paul suicide tmz video guy guy track don...</td>      <td>10</td>    </tr>    <tr>      <th>12</th>      <td>wars star jedi review movie cast opening luke ...</td>      <td>50</td>    </tr>    <tr>      <th>13</th>      <td>bowl super commercial 2018 puppy ad possible p...</td>      <td>19</td>    </tr>    <tr>      <th>14</th>      <td>snl franco kimmel james monologue jimmy natali...</td>      <td>25</td>    </tr>    <tr>      <th>15</th>      <td>kardashian kuwtk gender jenner kim khloe pregn...</td>      <td>29</td>    </tr>    <tr>      <th>16</th>      <td>cold open snl white house visit fox friends sa...</td>      <td>25</td>    </tr>    <tr>      <th>17</th>      <td>lip lip battle preview harmony fifth johnny jo...</td>      <td>35</td>    </tr>    <tr>      <th>18</th>      <td>live carpet red performance christmas story je...</td>      <td>11</td>    </tr>    <tr>      <th>19</th>      <td>ronan ronan plays close thr cast bird lady act...</td>      <td>45</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">title  <span class="comment">#聚类个数变多</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[&#39;movie justice justice talk review studios panther marvel avengers star&#39;,</span><br><span class="line"> &#39;new challenge wwhl stars ellen diy day 2017 life vs&#39;,</span><br><span class="line"> &#39;2017 voice amazon youtubers people movies jennifer music youtube 10&#39;,</span><br><span class="line"> &#39;christmas trump melania doctor state state tree donald thank capaldi&#39;,</span><br><span class="line"> &#39;2018 globes globes awards grammys sag moments hillary clinton mamma&#39;,</span><br><span class="line"> &#39;season hd tour kardashians keeping trailer grand episode fx ep&#39;,</span><br><span class="line"> &#39;graham graham dornan dornan perfect mic drop pitch jason helen&#39;,</span><br><span class="line"> &#39;official trailer hd netflix mirror black season sucks king date&#39;,</span><br><span class="line"> &#39;things searched searched wired stranger questions answers cast answer lower&#39;,</span><br><span class="line"> &#39;trailer official teaser hd world ready ocean paddington player theaters&#39;,</span><br><span class="line"> &#39;fear fear weird stuff box box box bearded cast dragons&#39;,</span><br><span class="line"> &#39;logan paul suicide tmz video guy guy track don diss&#39;,</span><br><span class="line"> &#39;wars star jedi review movie cast opening luke spoiler john&#39;,</span><br><span class="line"> &#39;bowl super commercial 2018 puppy ad possible possible devito danny&#39;,</span><br><span class="line"> &#39;snl franco kimmel james monologue jimmy natalie tommy tommy chastain&#39;,</span><br><span class="line"> &#39;kardashian kuwtk gender jenner kim khloe pregnancy baby kris family&#39;,</span><br><span class="line"> &#39;cold open snl white house visit fox friends santa trimming&#39;,</span><br><span class="line"> &#39;lip lip battle preview harmony fifth johnny johnny slays channels&#39;,</span><br><span class="line"> &#39;live carpet red performance christmas story jedi wars cast star&#39;,</span><br><span class="line"> &#39;ronan ronan plays close thr cast bird lady actresses director&#39;]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
        
    
    </summary>
    
      <category term="数据流" scheme="http://Raina-fighting.github.io/categories/%E6%95%B0%E6%8D%AE%E6%B5%81/"/>
    
      <category term="聚类" scheme="http://Raina-fighting.github.io/categories/%E6%95%B0%E6%8D%AE%E6%B5%81/%E8%81%9A%E7%B1%BB/"/>
    
    
      <category term="python" scheme="http://Raina-fighting.github.io/tags/python/"/>
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
      <category term="数据流" scheme="http://Raina-fighting.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/"/>
    
      <category term="聚类" scheme="http://Raina-fighting.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>典型社交媒体下华为粉丝网络的社区发现和链路预测</title>
    <link href="http://Raina-fighting.github.io/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/"/>
    <id>http://Raina-fighting.github.io/2019/08/15/典型社交媒体下华为粉丝网络的社区发现和链路预测/</id>
    <published>2019-08-15T10:51:00.000Z</published>
    <updated>2019-09-10T08:23:04.891Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">典型社交媒体下华为粉丝网络的社区发现和链路预测</h1><h2 id="分析提要"><a href="#分析提要" class="headerlink" title="分析提要"></a>分析提要</h2><ul><li>本案例使用的网络<a href="https://www.kaggle.com/andrewlucci/huawei-social-network-data" target="_blank" rel="noopener" title=" 数据下载自kaggle平台"> 1 </a>是通过爬取社交媒体平台来收集的（即 Facebook，Twitter 和 Instagram 华为页面）。API 提取 Facebook 帖子，Twitter 推文和 Instagram 帖子和评论。这是每天使用社交媒体的人之间的通信网络。通信时相互的，所以在此简化成无向无权网络；数据均来源于相关华为页面，必然是会提及或关心华为的群体，在此姑且称之为华为粉丝网络。本案例主要分析了 Instagram 通信网络结构，其中含有 1000 个节点和 4933 个边，预先对全部数据构建了网络，发现网络是连通的，从不同的角度展示网络未发现直观的规律，最终决定随机抽取 100 个用户深入挖掘他们之间的关系。</li></ul><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/network.png" alt="png"></p><h2 id="华为粉丝网络描述统计"><a href="#华为粉丝网络描述统计" class="headerlink" title="华为粉丝网络描述统计"></a>华为粉丝网络描述统计</h2><p>网络邻接矩阵可以使用多种方式展示。如图一，上图为网络图，使用的布局是kamada kawai，每个节点为一个用户，每一条边表明连接的用户间进行了通信。随机选择的100个用户间关联不是很紧密，存在一些小的群体，可能形成社区。下图使用热力图表达网络，颜色深的表示用户间有连接，可以看到用户边的存在较为稀疏，没有用户可以直接跟有其他很多用户有关联。其中删除了节点度为0的点，保留71个节点56条边。</p><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image005.jpg" alt="png"></p><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image006.jpg" alt="img"></p><center>图1：instagram数据的网络图和热力图展示</center><p>下面从度分布、节点中心性、边介数和凝聚性四个方面分析该网络存在的特征。</p><p><strong>（一）度分布</strong></p><p>对于无向无权网络，节点的度表示与节点连接的边的个数，度越大，在某种意义上该节点越重要。由图2，度分布图可知，网络大部分节点的度很小，88.7%的节点的度小于等于2，可判断符合现实网络的无标度特性。</p><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image007.jpg" alt="img"></p><center>图2：度分布图</center><p><strong>（二）节点中心性</strong></p><p>节点中心性定义了网络中一个节点的重要性。节点中心性度量有：节点度中心性、边介数中心性以及特征向量中心性。其中，使用最为广泛的是节点度中心性，即与点相连的边的数量。边介数中心性指的是一个结点担任其它两个结点之间最短路的桥梁的次数。一个结点充当“中介”的次数越高，它的中介中心度就越大。特征向量中心性认为，拥有很多的邻居的节点并不能确保这个节点就是重要的，拥有更多重要的邻居才能提供更有力的信息。</p><p>图3展示了节点中心性指标计算的结果。边介数中心性和特征向量中心性选出的重要用户相似性较大，主要由本网络存在的一个明显较大的连通分支影响。度中心性选择的用户则覆盖到更多的连通分支范围。</p><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image009.jpg" alt="img"></p><center>图3：不同节点中心性指标衡量的网络图</center><p><strong>（三）边介数</strong></p><p>边介数定义为网络中所有最短路径中经过该边的路径的数目占最短路径总数的比例。图4黄色的粗线为边介数前10的边。可以看到，仍然存在于大的连通分支中，由此很有可能在社区发现中这个大的分支会被拆开。</p><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image010.jpg" alt="img"></p><center>图4：边介数大的边在网络中的位置</center><p><strong>（四）凝聚性</strong></p><p>团是一类完全子图，集合内所有节点都由边相互连接，因而是完全凝聚的节点子集。本网络不存在尺寸大于2的子图。</p><p>网络的密度为实际出现的边和可能的边的频数之比，形容网络的结构复杂程度，越大说明网络越复杂，说明网络越能聚类块；聚类系数是对全局聚集性的度量，定义为连通三元组闭合形成三角形的相对频率，可以衡量网络中关联性如何，值越大代表交互关系越大，说明网络越复杂，越能聚类。从表1可以看出，该网络的网络密度为0.023，边聚类系数为0，网络总体聚集性不是很高，每个人联系过的其他人不会再有联系，每个人只会从一个来源转载评论同一篇帖子。网络的平均长度为4.026，符合小世界现象。</p><center>表1：密度、连通性指标</center><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image002.png" alt="img"></p><p>选择Neur为中心点，进行扩散，所能到达的点和距离。一步以内的点有5个，最远可触及的需要6步，见图5，体现在信息传递中，扩散往往是由大规模普及凝聚到小支点渗透的过程。在网络通信中，往往是存在几个影响力较大的点独立同步扩散中，这些点在本文中更倾向于度中心性选择的点。</p><p>​                                       <img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image012.png" alt="img"><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image013.jpg" alt="img"></p><center>图5：以Neur为中心点的扩散路径及距离</center><h2 id="华为粉丝网络社区发现"><a href="#华为粉丝网络社区发现" class="headerlink" title="华为粉丝网络社区发现"></a>华为粉丝网络社区发现</h2><p>本案例选择 GN 算法、随机游走、贪心算法和标签扩散算法实现华为粉丝网络的社区发现，其中 GN 算法是分裂算法，其他三者均是凝聚算法。GN 算法旨在最小化社区间连边的边介数；随机游走最大化社区间的流距离；贪心算法使用社区合并算法来快速搜索最大的模块度；标签扩散算法是每个节点取邻居中最流行的标签，达到迭代式收敛。</p><p>社区发现中 GN 算法和贪心算法探测到 16 个社区，基本上是一个连通分支一个社区，但本网络本身聚集性较差，基于凝聚思想的贪心算法要更快。模块度最低的是标签扩散算法，划分了更多的社区，对于本网络并不适用，且由于算法的随机性，每次划分的情况并不相同。</p><p><img src="/2019/08/15/%E5%85%B8%E5%9E%8B%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E4%B8%8B%E5%8D%8E%E4%B8%BA%E7%B2%89%E4%B8%9D%E7%BD%91%E7%BB%9C%E7%9A%84%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B/image014.png" alt="img"></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">layout(matrix(c(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),nr = <span class="number">2</span>, byrow = <span class="literal">T</span>))</span><br><span class="line">community_detection = list()</span><br><span class="line">set.seed(<span class="number">45</span>)</span><br><span class="line">l=layout.kamada.kawai(g) <span class="comment">#g为igraph类型网络</span></span><br><span class="line"></span><br><span class="line">system.time(ec &lt;- edge.betweenness.community(g,weight=E(g)$weight,directed=<span class="literal">F</span>))</span><br><span class="line">print(modularity(ec))</span><br><span class="line">plot(ec, g,main = paste(<span class="string">"edge.betweenness \n groups ="</span>,length(ec),sep = <span class="string">" "</span>),vertex.label=<span class="literal">NA</span>,vertex.size = <span class="number">2</span>,layout=l)</span><br><span class="line">community_detection$edge.betweenness = data.frame(len = length(ec),Q = modularity(ec))</span><br><span class="line"></span><br><span class="line">system.time(wc &lt;- walktrap.community(g,weights=E(g)$weight,step=<span class="number">4</span>))</span><br><span class="line"><span class="comment">#step代表游走步长，越大代表分类越粗糙，分类类别越小。默认为4.</span></span><br><span class="line">print(modularity(wc))</span><br><span class="line">plot(wc , g,main = paste(<span class="string">"walktrap \n groups ="</span>,length(wc),sep = <span class="string">" "</span>),vertex.label=<span class="literal">NA</span>,vertex.label=<span class="literal">NA</span>,vertex.size = <span class="number">2</span>,layout=l)</span><br><span class="line">community_detection$walktrap = data.frame(len = length(wc),Q = modularity(wc))</span><br><span class="line"></span><br><span class="line">system.time(fc &lt;- fastgreedy.community(g))</span><br><span class="line"><span class="comment"># length(fc) #发现社团个数</span></span><br><span class="line"><span class="comment"># sizes(fc) #每个社团包含的节点数</span></span><br><span class="line">print(modularity(fc))</span><br><span class="line">plot(fc, g,main = paste(<span class="string">"fastgreedy \n groups ="</span>,length(fc),sep = <span class="string">" "</span>),vertex.label=<span class="literal">NA</span>,vertex.label=<span class="literal">NA</span>,vertex.size = <span class="number">2</span>,layout=l)</span><br><span class="line">community_detection$fastgreedy = data.frame(len = length(fc),Q = modularity(fc))</span><br><span class="line"></span><br><span class="line">system.time(mc &lt;- multilevel.community(g))</span><br><span class="line">print(modularity(mc))</span><br><span class="line">plot(mc, g,main = paste(<span class="string">"mulstilevel \n groups ="</span>,length(mc),sep = <span class="string">" "</span>),vertex.label=<span class="literal">NA</span>,vertex.label=<span class="literal">NA</span>,vertex.size = <span class="number">2</span>,layout=l)</span><br><span class="line">community_detection$multilevel = data.frame(len = length(mc),Q = modularity(mc))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
        
    
    </summary>
    
      <category term="社交网络" scheme="http://Raina-fighting.github.io/categories/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="社交网络" scheme="http://Raina-fighting.github.io/tags/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C/"/>
    
      <category term="R" scheme="http://Raina-fighting.github.io/tags/R/"/>
    
      <category term="社区发现" scheme="http://Raina-fighting.github.io/tags/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0/"/>
    
      <category term="可视化" scheme="http://Raina-fighting.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>flowers数据集分类挖掘</title>
    <link href="http://Raina-fighting.github.io/2019/08/12/flowers%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%E6%8C%96%E6%8E%98/"/>
    <id>http://Raina-fighting.github.io/2019/08/12/flowers数据集分类挖掘/</id>
    <published>2019-08-12T10:51:00.000Z</published>
    <updated>2019-09-10T03:43:01.849Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">flowers数据集分类挖掘</h1><h2 id="分析提要"><a href="#分析提要" class="headerlink" title="分析提要"></a>分析提要</h2><ul><li><p>数据集中含有daisy、dandelion、sunflowers三种类型花的图片，希望通过合适的算法进行分类。本案例中使用卷积神经网络（sequential依次添加各层、在原有网络结构上生成新的模型）方法，对flowers数据进行训练，并计算预测准确率。</p></li><li><p>数据集中包含三种花卉类型，各种花卉的色彩、场景的噪点较多，同种花卉大小不一、角度不同、颜色不同。</p></li></ul><p><img src="/2019/08/12/flowers%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%E6%8C%96%E6%8E%98/flower.png" alt="png"></p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="展示训练集图片"><a href="#展示训练集图片" class="headerlink" title="展示训练集图片"></a>展示训练集图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_images_labels_prediction</span><span class="params">(images,labels,prediction,idx,num=<span class="number">10</span>)</span>:</span></span><br><span class="line">    fig = plt.gcf()</span><br><span class="line">    fig.set_size_inches(<span class="number">12</span>, <span class="number">14</span>)</span><br><span class="line">    <span class="keyword">if</span> num&gt;<span class="number">25</span>: num=<span class="number">25</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num):</span><br><span class="line">        ax=plt.subplot(<span class="number">5</span>,<span class="number">5</span>, <span class="number">1</span>+i)</span><br><span class="line">        ax.imshow(images[idx],cmap=<span class="string">'binary'</span>)</span><br><span class="line">                </span><br><span class="line">        title=str(i)+<span class="string">','</span>+label_dict[labels[i]]</span><br><span class="line">        <span class="keyword">if</span> len(prediction)&gt;<span class="number">0</span>:</span><br><span class="line">            title+=<span class="string">'=&gt;'</span>+label_dict[prediction[i]]</span><br><span class="line">            </span><br><span class="line">        ax.set_title(title,fontsize=<span class="number">10</span>) </span><br><span class="line">        ax.set_xticks([]);ax.set_yticks([])        </span><br><span class="line">        idx+=<span class="number">1</span> </span><br><span class="line">    plt.show()</span><br><span class="line">label_dict=&#123;<span class="number">0</span>:<span class="string">"dsy"</span>,<span class="number">1</span>:<span class="string">"ddl"</span>,<span class="number">2</span>:<span class="string">"sfl"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="string">"dsy"</span>,<span class="string">"ddl"</span>,<span class="string">"sfl"</span>]:</span><br><span class="line">    dire=[target +<span class="string">'train\\'</span>+i+<span class="string">'\\'</span>+ r <span class="keyword">for</span> r <span class="keyword">in</span> os.listdir(target + <span class="string">'train/'</span>+i)]</span><br><span class="line">    img=[load_img(i) <span class="keyword">for</span> i <span class="keyword">in</span> dire]</span><br><span class="line">    imgnew=[x.resize((<span class="number">128</span>,<span class="number">128</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> img]</span><br><span class="line">    x=[img_to_array(x) <span class="keyword">for</span> x <span class="keyword">in</span> imgnew]</span><br><span class="line">    x_img_dsytrain=np.array(x,dtype=int)</span><br><span class="line">    y_label_dsytrain=np.repeat(lk.index(i),x_img_dsytrain.shape[<span class="number">0</span>])</span><br><span class="line">    plot_images_labels_prediction(x_img_dsytrain,y_label_dsytrain,[],<span class="number">0</span>,num=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="图片预处理"><a href="#图片预处理" class="headerlink" title="图片预处理"></a>图片预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''对文件夹内所有图片处理；生成经过数据提升/归一化后的数据'''</span></span><br><span class="line"><span class="comment"># 图片尺寸</span></span><br><span class="line">img_width, img_height = <span class="number">128</span>, <span class="number">128</span></span><br><span class="line">input_shape = (img_width, img_height, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">train_data_dir = target + <span class="string">'train'</span></span><br><span class="line">validation_data_dir = target + <span class="string">'validation'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成变形图片</span></span><br><span class="line">train_pic_gen = ImageDataGenerator(</span><br><span class="line">        rescale=<span class="number">1.</span>/<span class="number">255</span>, <span class="comment"># 对输入图片归一化到0-1区间</span></span><br><span class="line">        rotation_range=<span class="number">20</span>, <span class="comment">#随机旋转角度的范围</span></span><br><span class="line">        width_shift_range=<span class="number">0.2</span>, <span class="comment">#随机水平移动的范围</span></span><br><span class="line">        height_shift_range=<span class="number">0.2</span>, <span class="comment">#随机竖直移动的范围</span></span><br><span class="line">        shear_range=<span class="number">0.2</span>, <span class="comment">#裁剪程度</span></span><br><span class="line">        zoom_range=<span class="number">0.5</span>, <span class="comment">#随机局部放大的程度</span></span><br><span class="line">        horizontal_flip=<span class="literal">True</span>, <span class="comment"># 水平翻转</span></span><br><span class="line">        fill_mode=<span class="string">'nearest'</span>) <span class="comment">#填充新像素方式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集不做变形处理，只需要归一化。</span></span><br><span class="line">validation_pic_gen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按文件夹生成训练集流和标签，binary：二分类；categorical：多分类</span></span><br><span class="line">train_flow = train_pic_gen.flow_from_directory(</span><br><span class="line">        train_data_dir,</span><br><span class="line">        target_size=(img_width, img_height),</span><br><span class="line">        batch_size=<span class="number">32</span>, <span class="comment">#batch的数据的大小</span></span><br><span class="line">        class_mode=<span class="string">'categorical'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#按文件夹生成测试集流和标签</span></span><br><span class="line">validation_flow = validation_pic_gen.flow_from_directory(</span><br><span class="line">        validation_data_dir,</span><br><span class="line">        target_size=(img_width, img_height),</span><br><span class="line">        batch_size=<span class="number">32</span>,</span><br><span class="line">        class_mode=<span class="string">'categorical'</span>)<span class="comment">#categorical</span></span><br></pre></td></tr></table></figure><h2 id="卷积神经网络（依次添加各层）"><a href="#卷积神经网络（依次添加各层）" class="headerlink" title="卷积神经网络（依次添加各层）"></a>卷积神经网络（依次添加各层）</h2><ul><li>图像的特征提取：通过卷积层、池化层的处理，提取图像的特征                                                                         </li><li>完全连接神经网络：包含平坦层、隐藏层、输出层                                                                                       </li><li>其中加入Dropout，可以在每次训练迭代时，随机地在神经网络结构中放弃部分神经元，以避免过度拟合</li></ul><h3 id="模型建立与预测"><a href="#模型建立与预测" class="headerlink" title="模型建立与预测"></a>模型建立与预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''模型建立'''</span></span><br><span class="line">model1 = Sequential([</span><br><span class="line">Convolution2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), input_shape=input_shape, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>),</span><br><span class="line">Dropout(<span class="number">0.25</span>),</span><br><span class="line">MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">Convolution2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>),</span><br><span class="line">Dropout(<span class="number">0.25</span>),</span><br><span class="line">MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">Flatten(),</span><br><span class="line">Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">Dropout(<span class="number">0.25</span>),</span><br><span class="line">Dense(<span class="number">3</span>, activation=<span class="string">'softmax'</span>),</span><br><span class="line">])</span><br><span class="line">model1.summary()</span><br><span class="line"><span class="comment"># 损失函数设置为二分类交叉熵</span></span><br><span class="line">model1.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">train2=model1.fit_generator(</span><br><span class="line">        train_flow,</span><br><span class="line">        steps_per_epoch=<span class="number">50</span>,</span><br><span class="line">        epochs=<span class="number">10</span>,</span><br><span class="line">        validation_data=validation_flow,</span><br><span class="line">        validation_steps=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_train_history</span><span class="params">(train_acc,test_acc)</span>:</span></span><br><span class="line">    plt.plot(train2.history[train_acc])</span><br><span class="line">    plt.plot(train2.history[test_acc])</span><br><span class="line">    plt.title(<span class="string">'Train History'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">    <span class="comment">#plt.xlabel('Epoch')</span></span><br><span class="line">    plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">show_train_history(<span class="string">'acc'</span>,<span class="string">'val_acc'</span>)</span><br><span class="line">show_train_history(<span class="string">'loss'</span>,<span class="string">'val_loss'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''模型预测'''</span></span><br><span class="line">prediction=model1.predict_generator(validation_flow,steps=<span class="number">1</span>,verbose=<span class="number">1</span>)</span><br><span class="line">predictionmax= np.argmax(prediction, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#预测准确率</span></span><br><span class="line">score=model1.evaluate_generator(validation_flow,steps=<span class="number">1</span>,verbose=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#查看混淆矩阵</span></span><br><span class="line">pd.crosstab(y_label,predictionmax,</span><br><span class="line">            rownames=[<span class="string">'label'</span>],colnames=[<span class="string">'predict'</span>])</span><br></pre></td></tr></table></figure><blockquote><p>获取数据可留言笔者</p></blockquote>]]></content>
    
    <summary type="html">
    
        
    
    </summary>
    
      <category term="图像分类" scheme="http://Raina-fighting.github.io/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="图像分类" scheme="http://Raina-fighting.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
      <category term="神经网络" scheme="http://Raina-fighting.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="python" scheme="http://Raina-fighting.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>中文文本处理</title>
    <link href="http://Raina-fighting.github.io/2019/08/10/%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"/>
    <id>http://Raina-fighting.github.io/2019/08/10/中文文本处理/</id>
    <published>2019-08-10T10:51:00.000Z</published>
    <updated>2019-09-09T15:53:11.797Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">中文文本处理</h1><blockquote><p><strong>本研究系同学共同成果，此部分仅限分词代码</strong></p></blockquote><ul><li>结巴模块分词训练<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="string">'''jieba加载词典'''</span></span><br><span class="line">jieba.load_userdict(<span class="string">"userdict.txt"</span>) <span class="comment"># 添加自定义用户词典，确保一些专有名词不会被错分，注意要修改为UTF-8编码格式</span></span><br><span class="line"><span class="string">'''加载停用词表'''</span></span><br><span class="line">stop = open(<span class="string">r'stopwords.txt'</span>, encoding=<span class="string">'UTF-8'</span>) <span class="comment"># 加载自定义的停用词表，删去一些虚词、助词、标点符号以及对分析任务没有实质性贡献的频繁出现的词（例如数据）[1]</span></span><br><span class="line">content = stop.read()</span><br><span class="line">stopwords = re.split(<span class="string">'\n'</span>,content)</span><br><span class="line"><span class="string">'''jieba分词'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">jieba_get_cut</span><span class="params">(data)</span>:</span></span><br><span class="line">    seg_list = list(jieba.cut(data.lower(),cut_all = <span class="literal">False</span>)) </span><br><span class="line">    seg_list_nolinebreak = [i <span class="keyword">for</span> i <span class="keyword">in</span> seg_list <span class="keyword">if</span> i != <span class="string">'\n'</span> <span class="keyword">and</span> i != <span class="string">' '</span>] </span><br><span class="line">    after_delect_stopwords = [i <span class="keyword">for</span> i <span class="keyword">in</span> seg_list_nolinebreak <span class="keyword">if</span> (i <span class="keyword">not</span> <span class="keyword">in</span> stopwords) | len(i) &gt;=<span class="number">2</span> ] </span><br><span class="line">    after_delect_num = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> after_delect_stopwords:</span><br><span class="line">         after_delect_num.append(re.sub(<span class="string">'[a-zA-Z0-9]'</span>, <span class="string">""</span>, i))</span><br><span class="line">    after_delect_num = [i <span class="keyword">for</span> i <span class="keyword">in</span> after_delect_num <span class="keyword">if</span> i <span class="keyword">is</span> <span class="keyword">not</span> <span class="string">''</span>]</span><br><span class="line">    <span class="keyword">return</span> after_delect_num</span><br></pre></td></tr></table></figure></li></ul><ul><li>补充：加载搜狗细胞词库为自定义用户词典</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''从github上找到的将搜狗的scel细胞词库文件转换成txt词典的程序 [2] '''</span></span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 拼音表偏移，</span></span><br><span class="line">startPy = <span class="number">0x1540</span>;</span><br><span class="line"><span class="comment"># 汉语词组表偏移</span></span><br><span class="line">startChinese = <span class="number">0x2628</span>;</span><br><span class="line"><span class="comment"># 全局拼音表</span></span><br><span class="line">GPy_Table = &#123;&#125;</span><br><span class="line"><span class="comment"># 解析结果</span></span><br><span class="line"><span class="comment"># 元组(词频,拼音,中文词组)的列表</span></span><br><span class="line">GTable = []</span><br><span class="line"><span class="comment"># 原始字节码转为字符串</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">byte2str</span><span class="params">(data)</span>:</span></span><br><span class="line">    pos = <span class="number">0</span></span><br><span class="line">    str = <span class="string">''</span></span><br><span class="line">    <span class="keyword">while</span> pos &lt; len(data):</span><br><span class="line">        c = chr(struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> c != chr(<span class="number">0</span>):</span><br><span class="line">            str += c</span><br><span class="line">        pos += <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> str</span><br><span class="line"><span class="comment"># 获取拼音表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPyTable</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = data[<span class="number">4</span>:]</span><br><span class="line">    pos = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> pos &lt; len(data):</span><br><span class="line">        index = struct.unpack(<span class="string">'H'</span>, bytes([data[pos],data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">        pos += <span class="number">2</span></span><br><span class="line">        lenPy = struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">        pos += <span class="number">2</span></span><br><span class="line">        py = byte2str(data[pos:pos + lenPy])</span><br><span class="line">        GPy_Table[index] = py</span><br><span class="line">        pos += lenPy</span><br><span class="line"><span class="comment"># 获取一个词组的拼音</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getWordPy</span><span class="params">(data)</span>:</span></span><br><span class="line">    pos = <span class="number">0</span></span><br><span class="line">    ret = <span class="string">''</span></span><br><span class="line">    <span class="keyword">while</span> pos &lt; len(data):</span><br><span class="line">        index = struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">        ret += GPy_Table[index]</span><br><span class="line">        pos += <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"><span class="comment"># 读取中文表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getChinese</span><span class="params">(data)</span>:</span></span><br><span class="line">    pos = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> pos &lt; len(data):</span><br><span class="line">        <span class="comment"># 同音词数量</span></span><br><span class="line">        same = struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 拼音索引表长度</span></span><br><span class="line">        pos += <span class="number">2</span></span><br><span class="line">        py_table_len = struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 拼音索引表</span></span><br><span class="line">        pos += <span class="number">2</span></span><br><span class="line">        py = getWordPy(data[pos: pos + py_table_len])</span><br><span class="line">        <span class="comment"># 中文词组</span></span><br><span class="line">        pos += py_table_len</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(same):</span><br><span class="line">            <span class="comment"># 中文词组长度</span></span><br><span class="line">            c_len = struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 中文词组</span></span><br><span class="line">            pos += <span class="number">2</span></span><br><span class="line">            word = byte2str(data[pos: pos + c_len])</span><br><span class="line">            <span class="comment"># 扩展数据长度</span></span><br><span class="line">            pos += c_len</span><br><span class="line">            ext_len = struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 词频</span></span><br><span class="line">            pos += <span class="number">2</span></span><br><span class="line">            count = struct.unpack(<span class="string">'H'</span>, bytes([data[pos], data[pos + <span class="number">1</span>]]))[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 保存</span></span><br><span class="line">            GTable.append((count, py, word))</span><br><span class="line">            <span class="comment"># 到下个词的偏移位置</span></span><br><span class="line">            pos += ext_len</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scel2txt</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    <span class="comment"># 分隔符</span></span><br><span class="line">    print(<span class="string">'-'</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="comment"># 读取文件</span></span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.read()</span><br><span class="line">    print(<span class="string">"词库名："</span>, byte2str(data[<span class="number">0x130</span>:<span class="number">0x338</span>])) <span class="comment"># .encode('GB18030')</span></span><br><span class="line">    print(<span class="string">"词库类型："</span>, byte2str(data[<span class="number">0x338</span>:<span class="number">0x540</span>]))</span><br><span class="line">    print(<span class="string">"描述信息："</span>, byte2str(data[<span class="number">0x540</span>:<span class="number">0xd40</span>]))</span><br><span class="line">    print(<span class="string">"词库示例："</span>, byte2str(data[<span class="number">0xd40</span>:startPy]))</span><br><span class="line">    getPyTable(data[startPy:startChinese])</span><br><span class="line">    getChinese(data[startChinese:])</span><br><span class="line"><span class="string">'''举例调用函数'''</span></span><br><span class="line">scel2txt(<span class="string">"IT计算机.scel"</span>) </span><br><span class="line">f = open(<span class="string">'./it_dict.txt'</span>, <span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> count, py, word <span class="keyword">in</span> GTable:</span><br><span class="line">    f.write( word + <span class="string">'\n'</span>) <span class="comment">#选择词汇写入utf-8的txt</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><p>[2]:  <a href="https://blog.csdn.net/cqdiy/article/details/82840027" target="_blank" rel="noopener">https://blog.csdn.net/cqdiy/article/details/82840027</a>    “”转载自博文作者寒江共血””</p>]]></content>
    
    <summary type="html">
    
        
    
    </summary>
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/categories/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
    
      <category term="python" scheme="http://Raina-fighting.github.io/tags/python/"/>
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
      <category term="分词" scheme="http://Raina-fighting.github.io/tags/%E5%88%86%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>YouTube网站的视频营销分析</title>
    <link href="http://Raina-fighting.github.io/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/"/>
    <id>http://Raina-fighting.github.io/2019/08/08/YouTube网站的视频营销分析/</id>
    <published>2019-08-08T10:51:46.000Z</published>
    <updated>2019-09-09T09:43:24.516Z</updated>
    
    <content type="html"><![CDATA[<h1 style="text-align:center">  基于YouTube网站的视频营销分析  </h1>## 一、背景介绍与数据描述  <p>热门YouTube视频  </p>+  YouTube[2]（世界著名的视频分享网站）维护着该平台上热门视频的列表。 根据Variety杂志的报道，“为了确定年度最热门的视频，YouTube使用了多种因素，包括衡量用户互动（观看次数，分享，评论和喜欢）。 请注意，它们不是整个日历年中观看次数最多的视频”。+  数据描述YouTube热门视频的每日记录[1]。包括美国、英国、德国、加拿大和法国（US，GB，DE，CA，FR）五个区域，每天最多可列出200个热门视频。+  每个区域的数据都在一个单独的文件中，数据字段包括视频标题、频道标题、发布时间、标签、观看次数、喜欢和不喜欢次数、评论次数和描述。数据还包括类别id，该字段在不同区域之间变化。 要检索特定视频的类别，需要在关联的JSON中找到它。+  本文对美国区域YouTube视频观看进行深入分析[1]: https://www.kaggle.com/datasnaek/youtube-new/data "通过YouTube API获取"[2]: https://github.com/DataSnaek/Trending-YouTube-Scraper<p>分析思路  </p>  1. 影响力分析--随时间的统计分析2. 情感分析 3. 主题分析  4. YouTube视频分类（待完善）5. 影响受欢迎程度的因素（待完善）<h2 id="二、描述分析"><a href="#二、描述分析" class="headerlink" title="二、描述分析"></a>二、描述分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_usa = pd.read_csv(<span class="string">"USvideos.csv"</span>)  <span class="comment">#(18973, 16)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 统一数据格式</span></span><br><span class="line">df_usa[<span class="string">'trending_date'</span>] = pd.to_datetime(df_usa[<span class="string">'trending_date'</span>], format=<span class="string">'%y.%d.%m'</span>)</span><br><span class="line">df_usa[<span class="string">'publish_time'</span>] = pd.to_datetime(df_usa[<span class="string">'publish_time'</span>], format=<span class="string">'%Y-%m-%dT%H:%M:%S.%fZ'</span>)</span><br><span class="line">df_usa.insert(<span class="number">5</span>, <span class="string">'publish_date'</span>, df_usa[<span class="string">'publish_time'</span>].dt.date)</span><br><span class="line">df_usa[<span class="string">'publish_time'</span>] = df_usa[<span class="string">'publish_time'</span>].dt.time</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 查看缺失</span></span><br><span class="line">df_usa.isnull().sum()/df_usa.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 数据描述分析</span></span><br><span class="line">df_usa.describe()</span><br><span class="line">df_usa.describe(include=[np.object])</span><br><span class="line">df_usa.describe(include=[np.bool])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(df_usa[<span class="string">'video_error_or_removed'</span>].loc[df_usa[<span class="string">'video_id'</span>]==<span class="string">'kZete48ZtsY'</span>])</span><br></pre></td></tr></table></figure><pre><code>14822    False15061    False15258    False15499     True15755     True15980     TrueName: video_error_or_removed, dtype: bool</code></pre><p>大多数视频不会存在[评论禁用][评级禁用][视频错误]的问题，少数视频可能如此，而甚少的视频在作为热门视频期间会发生状态的变化，如video id为kZete48ZtsY的视频在最后三天无法观看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_usa_single_day_trend=df_usa.drop_duplicates(subset=<span class="string">'video_id'</span>, keep=<span class="literal">False</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">df_usa_multiple_day_trend= df_usa.drop_duplicates(subset=<span class="string">'video_id'</span>,keep=<span class="string">'first'</span>,inplace=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">'出现在热门视频榜单一次'</span>,df_usa_single_day_trend.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'热门视频榜单保留第一次出现'</span>,df_usa_multiple_day_trend.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>出现在热门视频榜单一次 544热门视频榜单保留第一次出现 4079</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 首次上榜出现[评论禁用][评级禁用][视频错误]的视频</span></span><br><span class="line">print(df_usa_multiple_day_trend.describe(include=[np.bool]))</span><br></pre></td></tr></table></figure><pre><code>       comments_disabled ratings_disabled video_error_or_removedcount               4079             4079                   4079unique                 2                2                      2top                False            False                  Falsefreq                4015             4059                   4078</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 观看次数、喜欢和不喜欢次数、评论次数</span></span><br><span class="line">columns_show = [<span class="string">'views'</span>, <span class="string">'likes'</span>, <span class="string">'dislikes'</span>, <span class="string">'comment_count'</span>]</span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">corr = df_usa[columns_show].corr()</span><br><span class="line">sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(<span class="number">220</span>, <span class="number">10</span>, as_cmap=<span class="literal">True</span>),</span><br><span class="line">            square=<span class="literal">True</span>, ax=ax,annot=<span class="literal">True</span>) <span class="comment">#np.zeros_like构造一个新矩阵，矩阵的大小同corr</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_13_0.png" alt="png"></p><blockquote><p><strong>观看次数、喜欢和不喜欢次数、评论次数相关性</strong></p></blockquote><ul><li>评论数与不喜欢次数有较高的正相关，观众对不喜欢的视频会更多得表达自己的意见</li><li>观看次数与喜欢次数有较高的正相关，在观看前会进行选择，还是倾向选择符合个人爱好的视频</li><li>总体来看，四个字段都具有一些程度的正相关，往往比较热门的视频受众广泛，被观看得多，点赞与否定的也多，评论也多</li></ul><h2 id="三、影响力分析"><a href="#三、影响力分析" class="headerlink" title="三、影响力分析"></a>三、影响力分析</h2><h3 id="（一）热门视频时间趋势"><a href="#（一）热门视频时间趋势" class="headerlink" title="（一）热门视频时间趋势"></a>（一）热门视频时间趋势</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">date_count=df_usa.groupby([<span class="string">'trending_date'</span>])[<span class="string">'trending_date'</span>].agg(<span class="string">'count'</span>)</span><br><span class="line">print(<span class="string">'天数'</span>,len(date_count))</span><br><span class="line">print(<span class="string">'起始日期'</span>,date_count[<span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'结束日期'</span>,date_count[<span class="number">-1</span>:])</span><br><span class="line"><span class="comment">### 缺少1-10和1-11的数据</span></span><br></pre></td></tr></table></figure><pre><code>天数 95起始日期 trending_date2017-11-14    200Name: trending_date, dtype: int64结束日期 trending_date2018-02-18    200Name: trending_date, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">publish_hour=[<span class="number">0</span>]*<span class="number">24</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_usa_multiple_day_trend.index:</span><br><span class="line">    temp=df_usa_multiple_day_trend[<span class="string">'publish_time'</span>].loc[i]</span><br><span class="line">    publish_hour[temp.hour]+=<span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">9</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax=sns.barplot(x=np.arange(<span class="number">24</span>),y=publish_hour)</span><br><span class="line">plt.xlabel(<span class="string">"Publish_hour"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.title(<span class="string">"Best time to publish vide"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_19_1.png" alt="png"></p><h3 id="（二）热门视频发布天数"><a href="#（二）热门视频发布天数" class="headerlink" title="（二）热门视频发布天数"></a>（二）热门视频发布天数</h3><h4 id="连续出现在热门视频榜单天数"><a href="#连续出现在热门视频榜单天数" class="headerlink" title="连续出现在热门视频榜单天数"></a>连续出现在热门视频榜单天数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_usa_video_trend_maximum=df_usa.groupby(by=[<span class="string">'video_id'</span>],as_index=<span class="literal">False</span>).count().sort_values(by=<span class="string">'trending_date'</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">df_usa_video_trend_maximum[<span class="string">'trending_date'</span>].value_counts().plot.bar()</span><br><span class="line">plt.xticks(rotation=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]), &lt;a list of 14 Text xticklabel objects&gt;)</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_22_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df_usa_video_trend_maximum=df_usa.groupby(by=[<span class="string">'video_id'</span>],as_index=<span class="literal">False</span>).count().sort_values(by=<span class="string">'trending_date'</span>,ascending=<span class="literal">False</span>).head()</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax=sns.barplot(x=<span class="string">'video_id'</span>,y=<span class="string">'trending_date'</span>,data=df_usa_video_trend_maximum)</span><br><span class="line">plt.xlabel(<span class="string">"Video Id"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.title(<span class="string">"Top 5 Videos that trended maximum days in USA"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_23_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_usa[<span class="string">'publish_date'</span>]=pd.to_datetime(df_usa[<span class="string">'publish_date'</span>]) <span class="comment">#np.datetime64</span></span><br><span class="line">df_usa[<span class="string">'trending_date'</span>]=pd.to_datetime(df_usa[<span class="string">'trending_date'</span>])</span><br><span class="line">df_usa_multiple_day_trend= df_usa.drop_duplicates(subset=<span class="string">'video_id'</span>,keep=<span class="string">'first'</span>,inplace=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_usa_multiple_day_trend[<span class="string">'Days_taken'</span>]=<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_usa_multiple_day_trend.index:</span><br><span class="line">    df_usa_multiple_day_trend[<span class="string">'Days_taken'</span>].loc[i]=(df_usa_multiple_day_trend[<span class="string">'trending_date'</span>].loc[i]- df_usa_multiple_day_trend[<span class="string">'publish_date'</span>].loc[i])/ np.timedelta64(<span class="number">1</span>, <span class="string">'D'</span>)</span><br></pre></td></tr></table></figure><h4 id="从发布到成为热门视频需要天数"><a href="#从发布到成为热门视频需要天数" class="headerlink" title="从发布到成为热门视频需要天数"></a>从发布到成为热门视频需要天数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">usa_days=df_usa_multiple_day_trend.sort_values(by=<span class="string">'Days_taken'</span>,ascending=<span class="literal">False</span>).head(<span class="number">5</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax = sns.barplot(x=<span class="string">'title'</span>,y=<span class="string">'Days_taken'</span>, data=usa_days)</span><br><span class="line">plt.xlabel(<span class="string">"Video Title"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"No. of Days"</span>)</span><br><span class="line">plt.title(<span class="string">"Maximum no. of days taken by 5 videos to be popular in USA"</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(usa_days[[<span class="string">'video_id'</span>,<span class="string">'Days_taken'</span>]])</span><br></pre></td></tr></table></figure><pre><code>D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_27_1.png" alt="png"></p><pre><code>          video_id  Days_taken16294  MJO3FmmFuh4      4215.010710  UQtt9I6c-YM      3563.02311   wFEAoF7RC7Y      3448.02553   Y8JGfi4MJ8k      3398.01136   rO_mAQC9bv0      3176.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_usa_multiple_day_trend[<span class="string">'Days_taken'</span>].value_counts().plot.hist()</span><br><span class="line">plt.xticks(rotation=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>(array([-250.,    0.,  250.,  500.,  750., 1000., 1250., 1500., 1750.,        2000.]), &lt;a list of 10 Text xticklabel objects&gt;)</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_28_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_usa.loc[df_usa[<span class="string">'video_id'</span>]==<span class="string">'MJO3FmmFuh4'</span>]</span><br></pre></td></tr></table></figure><blockquote><p>Budweiser百威啤酒广告  </p><ul><li>whazzup</li></ul></blockquote><blockquote><blockquote><p>Wassup? Whassup? There is still no consensus on how to spell this.<br> –Budweiser <strong>adland<a href="https://adland.tv/adnews/budweiser-true-wassup-girlfriend-2000-030-usa" target="_blank" rel="noopener">1</a></strong></p></blockquote></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_usa[<span class="string">'description'</span>][<span class="number">16294</span>]</span><br></pre></td></tr></table></figure><pre><code>&#39;Original Whazzup ad - however, there is a little *glitch* in the middle...sorry.&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 发布当天成为热门视频个数</span></span><br><span class="line">usa_days=df_usa_multiple_day_trend.sort_values(by=<span class="string">'Days_taken'</span>,ascending=<span class="literal">True</span>)</span><br><span class="line">count=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> usa_days.index:</span><br><span class="line">    count=count+<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> usa_days[<span class="string">'Days_taken'</span>].loc[i]&gt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">print(<span class="string">'发布当天成为热门视频个数'</span>,count)</span><br></pre></td></tr></table></figure><pre><code>发布当天成为热门视频个数 105</code></pre><h3 id="（三）热门视频TOP频道"><a href="#（三）热门视频TOP频道" class="headerlink" title="（三）热门视频TOP频道"></a>（三）热门视频TOP频道</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">usa_trending_channel=df_usa_multiple_day_trend.groupby(by=[<span class="string">'channel_title'</span>],as_index=<span class="literal">False</span>).count().sort_values(by=<span class="string">'video_id'</span>,ascending=<span class="literal">False</span>).head()</span><br><span class="line"><span class="comment">#len(np.unique(df_usa_multiple_day_trend['channel_title']))  1796</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax = sns.barplot(x=<span class="string">'channel_title'</span>,y=<span class="string">'video_id'</span>, data=usa_trending_channel)</span><br><span class="line">plt.xlabel(<span class="string">"Channel Title"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.title(<span class="string">"Top 5 Trending Channel in USA"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(usa_trending_channel[[<span class="string">'channel_title'</span>,<span class="string">'video_id'</span>]])</span><br></pre></td></tr></table></figure><pre><code>D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_34_1.png" alt="png"></p><pre><code>                               channel_title  video_id1449  The Tonight Show Starring Jimmy Fallon        421194                              Refinery29        391464                            TheEllenShow        38714                        Jimmy Kimmel Live        361028                                 Netflix        36</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">df_channel=df_usa_multiple_day_trend[[<span class="string">'channel_title'</span>,<span class="string">'comment_count'</span>]]</span><br><span class="line">df_channel_comment=df_channel.groupby([<span class="string">'channel_title'</span>]).mean().sort_values(by=<span class="string">'comment_count'</span>,ascending=<span class="literal">False</span>).head()</span><br><span class="line">df_channel_comment</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax = sns.barplot(x=df_channel_comment.index,y=<span class="string">'comment_count'</span>, data=df_channel_comment)</span><br><span class="line">plt.xlabel(<span class="string">"Channel Title"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.title(<span class="string">"Top 5 Trending Channel in USA"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(df_channel_comment[[<span class="string">'comment_count'</span>]])</span><br></pre></td></tr></table></figure><pre><code>D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_35_1.png" alt="png"></p><pre><code>                   comment_countchannel_title                   Logan Paul Vlogs       416049.75ibighit                233969.00YouTube Spotlight      233550.50The ACE Family         179396.00Collins Key            116532.00</code></pre><h3 id="（四）热门视频TOP类别"><a href="#（四）热门视频TOP类别" class="headerlink" title="（四）热门视频TOP类别"></a>（四）热门视频TOP类别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">id_to_category = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"US_category_id.json"</span>,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    id_data = json.load(f)</span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> id_data[<span class="string">"items"</span>]:</span><br><span class="line">        id_to_category[category[<span class="string">"id"</span>]] = category[<span class="string">"snippet"</span>][<span class="string">"title"</span>]</span><br><span class="line"></span><br><span class="line">id_to_category</span><br></pre></td></tr></table></figure><pre><code>{&#39;1&#39;: &#39;Film &amp; Animation&#39;, &#39;10&#39;: &#39;Music&#39;, &#39;15&#39;: &#39;Pets &amp; Animals&#39;, &#39;17&#39;: &#39;Sports&#39;, &#39;18&#39;: &#39;Short Movies&#39;, &#39;19&#39;: &#39;Travel &amp; Events&#39;, &#39;2&#39;: &#39;Autos &amp; Vehicles&#39;, &#39;20&#39;: &#39;Gaming&#39;, &#39;21&#39;: &#39;Videoblogging&#39;, &#39;22&#39;: &#39;People &amp; Blogs&#39;, &#39;23&#39;: &#39;Comedy&#39;, &#39;24&#39;: &#39;Entertainment&#39;, &#39;25&#39;: &#39;News &amp; Politics&#39;, &#39;26&#39;: &#39;Howto &amp; Style&#39;, &#39;27&#39;: &#39;Education&#39;, &#39;28&#39;: &#39;Science &amp; Technology&#39;, &#39;29&#39;: &#39;Nonprofits &amp; Activism&#39;, &#39;30&#39;: &#39;Movies&#39;, &#39;31&#39;: &#39;Anime/Animation&#39;, &#39;32&#39;: &#39;Action/Adventure&#39;, &#39;33&#39;: &#39;Classics&#39;, &#39;34&#39;: &#39;Comedy&#39;, &#39;35&#39;: &#39;Documentary&#39;, &#39;36&#39;: &#39;Drama&#39;, &#39;37&#39;: &#39;Family&#39;, &#39;38&#39;: &#39;Foreign&#39;, &#39;39&#39;: &#39;Horror&#39;, &#39;40&#39;: &#39;Sci-Fi/Fantasy&#39;, &#39;41&#39;: &#39;Thriller&#39;, &#39;42&#39;: &#39;Shorts&#39;, &#39;43&#39;: &#39;Shows&#39;, &#39;44&#39;: &#39;Trailers&#39;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_usa_multiple_day_trend[<span class="string">"category_id"</span>] = df_usa_multiple_day_trend[<span class="string">"category_id"</span>].astype(str)</span><br><span class="line">df_usa_multiple_day_trend.insert(<span class="number">4</span>, <span class="string">"category"</span>,df_usa_multiple_day_trend[<span class="string">"category_id"</span>].map(id_to_category))</span><br></pre></td></tr></table></figure><pre><code>D:\jupyter\lib\site-packages\ipykernel\__main__.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value insteadSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy  if __name__ == &#39;__main__&#39;:</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">usa_category_id=df_usa_multiple_day_trend.groupby(by=[<span class="string">'category'</span>],as_index=<span class="literal">False</span>).count().sort_values(by=<span class="string">'video_id'</span>,ascending=<span class="literal">False</span>).head()</span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">7</span>))</span><br><span class="line"><span class="comment">#sns.kdeplot(usa_category_id['category_id']);</span></span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax = sns.barplot(x=<span class="string">'category'</span>,y=<span class="string">'video_id'</span>, data=usa_category_id)</span><br><span class="line">plt.xlabel(<span class="string">"Category"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.title(<span class="string">"Top 5 Category IDs for USA"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(usa_category_id[[<span class="string">'category'</span>,<span class="string">'video_id'</span>]])</span><br></pre></td></tr></table></figure><pre><code>D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_39_1.png" alt="png"></p><pre><code>          category  video_id3    Entertainment       9967            Music       5238  News &amp; Politics       3956    Howto &amp; Style       3721           Comedy       346</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_category=df_usa_multiple_day_trend[[<span class="string">'category'</span>,<span class="string">'likes'</span>,<span class="string">'dislikes'</span>]]</span><br><span class="line">df_category_iflike=df_category.groupby([<span class="string">'category'</span>]).mean()</span><br><span class="line">df_category_top=df_category_iflike.loc[[<span class="string">'Entertainment'</span>,<span class="string">'Music'</span>,<span class="string">'News &amp; Politics'</span>,<span class="string">'Howto &amp; Style'</span>,<span class="string">'Comedy'</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_category_top.plot()</span><br><span class="line">plt.xlabel(<span class="string">"Category"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.title(<span class="string">"Mean likes or dislikes of Top 5 Category"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_41_0.png" alt="png"></p><h2 id="四、情感分析"><a href="#四、情感分析" class="headerlink" title="四、情感分析"></a>四、情感分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> RegexpTokenizer</span><br><span class="line"><span class="keyword">from</span> stop_words <span class="keyword">import</span> get_stop_words <span class="comment">#!对比nltk的停词表</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> sent_tokenize, word_tokenize</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, STOPWORDS</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 统计标题词频</span></span><br><span class="line">a = df_usa[<span class="string">'title'</span>].str.lower().str.cat(sep=<span class="string">' '</span>) <span class="comment">#转换为空格连接的文本串</span></span><br><span class="line">b = re.sub(<span class="string">'[^A-Za-z]+'</span>, <span class="string">' '</span>, a) <span class="comment">#只保留了英文字母--^从开头匹配；+号多个匹配</span></span><br><span class="line">stop_words = list(get_stop_words(<span class="string">'en'</span>))         </span><br><span class="line">nltk_words = [<span class="string">'i'</span>, <span class="string">'me'</span>, <span class="string">'my'</span>, <span class="string">'myself'</span>, <span class="string">'we'</span>, <span class="string">'our'</span>, <span class="string">'ours'</span>, <span class="string">'ourselves'</span>, <span class="string">'you'</span>, <span class="string">'your'</span>, <span class="string">'yours'</span>, <span class="string">'yourself'</span>, <span class="string">'yourselves'</span>, <span class="string">'he'</span>, <span class="string">'him'</span>, <span class="string">'his'</span>, <span class="string">'himself'</span>, <span class="string">'she'</span>, <span class="string">'her'</span>, <span class="string">'hers'</span>, <span class="string">'herself'</span>, <span class="string">'it'</span>, <span class="string">'its'</span>, <span class="string">'itself'</span>, <span class="string">'they'</span>, <span class="string">'them'</span>, <span class="string">'their'</span>, <span class="string">'theirs'</span>, <span class="string">'themselves'</span>, <span class="string">'what'</span>, <span class="string">'which'</span>, <span class="string">'who'</span>, <span class="string">'whom'</span>, <span class="string">'this'</span>, <span class="string">'that'</span>, <span class="string">'these'</span>, <span class="string">'those'</span>, <span class="string">'am'</span>, <span class="string">'is'</span>, <span class="string">'are'</span>, <span class="string">'was'</span>, <span class="string">'were'</span>, <span class="string">'be'</span>, <span class="string">'been'</span>, <span class="string">'being'</span>, <span class="string">'have'</span>, <span class="string">'has'</span>, <span class="string">'had'</span>, <span class="string">'having'</span>, <span class="string">'do'</span>, <span class="string">'does'</span>, <span class="string">'did'</span>, <span class="string">'doing'</span>, <span class="string">'a'</span>, <span class="string">'an'</span>, <span class="string">'the'</span>, <span class="string">'and'</span>, <span class="string">'but'</span>, <span class="string">'if'</span>, <span class="string">'or'</span>, <span class="string">'because'</span>, <span class="string">'as'</span>, <span class="string">'until'</span>, <span class="string">'while'</span>, <span class="string">'of'</span>, <span class="string">'at'</span>, <span class="string">'by'</span>, <span class="string">'for'</span>, <span class="string">'with'</span>, <span class="string">'about'</span>, <span class="string">'against'</span>, <span class="string">'between'</span>, <span class="string">'into'</span>, <span class="string">'through'</span>, <span class="string">'during'</span>, <span class="string">'before'</span>, <span class="string">'after'</span>, <span class="string">'above'</span>, <span class="string">'below'</span>, <span class="string">'to'</span>, <span class="string">'from'</span>, <span class="string">'up'</span>, <span class="string">'down'</span>, <span class="string">'in'</span>, <span class="string">'out'</span>, <span class="string">'on'</span>, <span class="string">'off'</span>, <span class="string">'over'</span>, <span class="string">'under'</span>, <span class="string">'again'</span>, <span class="string">'further'</span>, <span class="string">'then'</span>, <span class="string">'once'</span>, <span class="string">'here'</span>, <span class="string">'there'</span>, <span class="string">'when'</span>, <span class="string">'where'</span>, <span class="string">'why'</span>, <span class="string">'how'</span>, <span class="string">'all'</span>, <span class="string">'any'</span>, <span class="string">'both'</span>, <span class="string">'each'</span>, <span class="string">'few'</span>, <span class="string">'more'</span>, <span class="string">'most'</span>, <span class="string">'other'</span>, <span class="string">'some'</span>, <span class="string">'such'</span>, <span class="string">'no'</span>, <span class="string">'nor'</span>, <span class="string">'not'</span>, <span class="string">'only'</span>, <span class="string">'own'</span>, <span class="string">'same'</span>, <span class="string">'so'</span>, <span class="string">'than'</span>, <span class="string">'too'</span>, <span class="string">'very'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'can'</span>, <span class="string">'will'</span>, <span class="string">'just'</span>, <span class="string">'don'</span>, <span class="string">'should'</span>, <span class="string">'now'</span>]</span><br><span class="line"><span class="comment">#nltk.download("stopwords") #！连接失败 http://www.nltk.org/nltk_data/ http://johnlaudun.org/20130126-nltk-stopwords/</span></span><br><span class="line">stop_words.extend(nltk_words)</span><br><span class="line">word_tokens = word_tokenize(b) <span class="comment">#nltk分词!tokenize</span></span><br><span class="line">filtered_word = [w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokens <span class="keyword">if</span> <span class="keyword">not</span> w <span class="keyword">in</span> stop_words] </span><br><span class="line">print(<span class="string">"nltk分词后词语个数"</span>,len(filtered_word))</span><br><span class="line"><span class="comment">## 去掉长度小于等于2词语</span></span><br><span class="line">processed_word = [word <span class="keyword">for</span> word <span class="keyword">in</span> filtered_word <span class="keyword">if</span> len(word) &gt; <span class="number">2</span>]</span><br><span class="line">print(<span class="string">"处理过后词语个数"</span>,len(processed_word))</span><br><span class="line">word_dist = nltk.FreqDist(processed_word)</span><br></pre></td></tr></table></figure><pre><code>nltk分词后词语个数 114272处理过后词语个数 108729</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">topn=<span class="number">100</span></span><br><span class="line">rslt = pd.DataFrame(word_dist.most_common(topn),</span><br><span class="line">                    columns=[<span class="string">'Word'</span>, <span class="string">'Frequency'</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax = sns.barplot(x=<span class="string">"Word"</span>,y=<span class="string">"Frequency"</span>, data=rslt.head(<span class="number">5</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_45_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 词云图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wc</span><span class="params">(data,bgcolor,title)</span>:</span></span><br><span class="line">    plt.figure(figsize = (<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">    wc = WordCloud(background_color = bgcolor, max_words = <span class="number">1000</span>,  max_font_size = <span class="number">50</span>)</span><br><span class="line">    wc.generate(<span class="string">' '</span>.join(data))</span><br><span class="line">    plt.imshow(wc)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">wc(processed_word,<span class="string">'black'</span>,<span class="string">'Common Words'</span> )</span><br></pre></td></tr></table></figure><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_46_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> textblob <span class="keyword">import</span> TextBlob</span><br><span class="line"><span class="comment">#https://planspace.org/20150607-textblob_sentiment/ 计算公式</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bloblist_title=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df_usa_multiple_day_trend[<span class="string">'title'</span>]:</span><br><span class="line">    blob = TextBlob(row)</span><br><span class="line">    bloblist_title.append((row,blob.sentiment.polarity, blob.sentiment.subjectivity))</span><br><span class="line">    df_usa_polarity_title = pd.DataFrame(bloblist_title, columns = [<span class="string">'sentence'</span>,<span class="string">'sentiment'</span>,<span class="string">'polarity'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f_title</span><span class="params">(df_usa_polarity_title)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> df_usa_polarity_title[<span class="string">'sentiment'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">        val = <span class="string">"Positive"</span></span><br><span class="line">    <span class="keyword">elif</span> df_usa_polarity_title[<span class="string">'sentiment'</span>] == <span class="number">0</span>:</span><br><span class="line">        val = <span class="string">"Neutral"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        val = <span class="string">"Negative"</span></span><br><span class="line">    <span class="keyword">return</span> val</span><br><span class="line"></span><br><span class="line">df_usa_polarity_title[<span class="string">'Sentiment_Type'</span>] = df_usa_polarity_title.apply(f_title, axis=<span class="number">1</span>)  <span class="comment">#df_usa_polarity_title.loc[14]['sentiment']</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_usa_polarity_title[<span class="string">'category'</span>]=list(df_usa_multiple_day_trend[<span class="string">'category'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=df_usa_polarity_title[[<span class="string">'Sentiment_Type'</span>,<span class="string">'sentiment'</span>]].groupby(<span class="string">'Sentiment_Type'</span>).count()</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">ax = sns.barplot(x=a.index,y=<span class="string">"sentiment"</span>, data=a)</span><br><span class="line">plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_50_1.png" alt="png"></p><h3 id="分类别视频情感分析"><a href="#分类别视频情感分析" class="headerlink" title="分类别视频情感分析"></a>分类别视频情感分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_usa_category_polar=df_usa_polarity_title[[<span class="string">'category'</span>,<span class="string">'Sentiment_Type'</span>,<span class="string">'sentiment'</span>]].groupby([<span class="string">'category'</span>,<span class="string">'Sentiment_Type'</span>]).count()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_usa_category_polar.plot.bar()</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1f60e7d1080&gt;</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_53_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="string">'Entertainment'</span>,<span class="string">'Music'</span>,<span class="string">'News &amp; Politics'</span>,<span class="string">'Howto &amp; Style'</span>,<span class="string">'Comedy'</span>]:</span><br><span class="line">    a=df_usa_category_polar.ix[[i]]</span><br><span class="line">    print(a)</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">    sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">    ax = sns.barplot(x=a.index.levels[<span class="number">1</span>],y=<span class="string">"sentiment"</span>, data=a)</span><br><span class="line">    plt.ylabel(<span class="string">"Count"</span>)</span><br><span class="line">    plt.title(i)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><pre><code>                              sentimentcategory      Sentiment_Type           Entertainment Negative              139              Neutral               559              Positive              298D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_54_2.png" alt="png"></p><pre><code>                         sentimentcategory Sentiment_Type           Music    Negative               54         Neutral               308         Positive              161D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_54_5.png" alt="png"></p><pre><code>                                sentimentcategory        Sentiment_Type           News &amp; Politics Negative               76                Neutral               230                Positive               89D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_54_8.png" alt="png"></p><pre><code>                              sentimentcategory      Sentiment_Type           Howto &amp; Style Negative               50              Neutral               205              Positive              117D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_54_11.png" alt="png"></p><pre><code>                         sentimentcategory Sentiment_Type           Comedy   Negative               51         Neutral               205         Positive               90D:\jupyter\lib\site-packages\seaborn\categorical.py:342: DeprecationWarning: pandas.core.common.is_categorical_dtype is deprecated. import from the public API: pandas.api.types.is_categorical_dtype instead  elif is_categorical(y):</code></pre><p><img src="/2019/08/08/YouTube%E7%BD%91%E7%AB%99%E7%9A%84%E8%A7%86%E9%A2%91%E8%90%A5%E9%94%80%E5%88%86%E6%9E%90/output_54_14.png" alt="png"></p><h2 id="五、主题探测"><a href="#五、主题探测" class="headerlink" title="五、主题探测"></a>五、主题探测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> LatentDirichletAllocation</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer </span><br><span class="line"><span class="keyword">import</span> pyLDAvis</span><br><span class="line"><span class="keyword">import</span> pyLDAvis.sklearn</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">entertainment_title= df_usa_multiple_day_trend[<span class="string">"title"</span>][(df_usa_multiple_day_trend[<span class="string">'category'</span>] == <span class="string">'Entertainment'</span>)] </span><br><span class="line">music_title= df_usa_multiple_day_trend[<span class="string">"title"</span>][(df_usa_multiple_day_trend[<span class="string">'category'</span>] == <span class="string">'Music'</span>)] </span><br><span class="line">news_politics_title= df_usa_multiple_day_trend[<span class="string">"title"</span>][(df_usa_multiple_day_trend[<span class="string">'category'</span>] == <span class="string">'News &amp; Politics'</span>)] </span><br><span class="line">style_title= df_usa_multiple_day_trend[<span class="string">"title"</span>][(df_usa_multiple_day_trend[<span class="string">'category'</span>] == <span class="string">'Howto &amp; Style'</span>)] </span><br><span class="line">comedy_title= df_usa_multiple_day_trend[<span class="string">"title"</span>][(df_usa_multiple_day_trend[<span class="string">'category'</span>] == <span class="string">'Comedy'</span>)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_entertainment_title = CountVectorizer(min_df=<span class="number">5</span>, max_df=<span class="number">0.9</span>, stop_words=<span class="string">'english'</span>, lowercase=<span class="literal">True</span>, token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line"><span class="comment">#token_pattern</span></span><br><span class="line">entertainment_title_vectorized = vectorizer_entertainment_title.fit_transform(entertainment_title)</span><br><span class="line">lda_popular_entertainment_title_vectorized = LatentDirichletAllocation(n_topics=<span class="number">7</span>, max_iter=<span class="number">5</span>, learning_method=<span class="string">'online'</span>)</span><br><span class="line">entertainment_title_vectorized_lda = lda_popular_entertainment_title_vectorized.fit_transform(entertainment_title_vectorized )</span><br><span class="line"></span><br><span class="line">dash = pyLDAvis.sklearn.prepare(lda_popular_entertainment_title_vectorized,entertainment_title_vectorized, vectorizer_entertainment_title, mds=<span class="string">'tsne'</span>)</span><br><span class="line">pyLDAvis.show(dash)</span><br></pre></td></tr></table></figure><p>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_music_title = CountVectorizer(min_df=<span class="number">5</span>, max_df=<span class="number">0.9</span>, stop_words=<span class="string">'english'</span>, lowercase=<span class="literal">True</span>, token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line">music_title_vectorized = vectorizer_music_title.fit_transform(music_title)</span><br><span class="line"></span><br><span class="line">lda_music_title_vectorized= LatentDirichletAllocation(n_topics=<span class="number">6</span>, max_iter=<span class="number">5</span>, learning_method=<span class="string">'online'</span>)</span><br><span class="line">music_title_vectorized_lda = lda_music_title_vectorized.fit_transform(music_title_vectorized )</span><br><span class="line"></span><br><span class="line">dash = pyLDAvis.sklearn.prepare(lda_music_title_vectorized,music_title_vectorized, vectorizer_music_title , mds=<span class="string">'tsne'</span>)</span><br><span class="line">pyLDAvis.show(dash)</span><br></pre></td></tr></table></figure><p>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_news_politics_title = CountVectorizer(min_df=<span class="number">5</span>, max_df=<span class="number">0.9</span>, stop_words=<span class="string">'english'</span>, lowercase=<span class="literal">True</span>, token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line">news_politics_title_vectorized = vectorizer_news_politics_title.fit_transform(news_politics_title)</span><br><span class="line">lda_news_politics_title_vectorized= LatentDirichletAllocation(n_topics=<span class="number">7</span>, max_iter=<span class="number">5</span>, learning_method=<span class="string">'online'</span>,verbose=<span class="literal">True</span>)</span><br><span class="line">news_politics_title_vectorized_lda = lda_news_politics_title_vectorized.fit_transform(news_politics_title_vectorized )</span><br><span class="line"></span><br><span class="line">dash = pyLDAvis.sklearn.prepare(lda_news_politics_title_vectorized,news_politics_title_vectorized, vectorizer_news_politics_title , mds=<span class="string">'tsne'</span>)</span><br><span class="line">pyLDAvis.show(dash)</span><br></pre></td></tr></table></figure><p>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_style_title = CountVectorizer(min_df=<span class="number">5</span>, max_df=<span class="number">0.9</span>, stop_words=<span class="string">'english'</span>, lowercase=<span class="literal">True</span>, token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line">style_title_vectorized = vectorizer_style_title.fit_transform(style_title)</span><br><span class="line">lda_style_title_vectorized= LatentDirichletAllocation(n_topics=<span class="number">7</span>, max_iter=<span class="number">5</span>, learning_method=<span class="string">'online'</span>,verbose=<span class="literal">True</span>)</span><br><span class="line">style_title_vectorized_lda = lda_style_title_vectorized.fit_transform(style_title_vectorized )</span><br><span class="line"></span><br><span class="line">dash = pyLDAvis.sklearn.prepare(lda_style_title_vectorized,style_title_vectorized, vectorizer_style_title , mds=<span class="string">'tsne'</span>)</span><br><span class="line">pyLDAvis.show(dash)</span><br></pre></td></tr></table></figure><p>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vectorizer_comedy_title = CountVectorizer(min_df=<span class="number">5</span>, max_df=<span class="number">0.9</span>, stop_words=<span class="string">'english'</span>, lowercase=<span class="literal">True</span>, token_pattern=<span class="string">'[a-zA-Z\-][a-zA-Z\-]&#123;2,&#125;'</span>)</span><br><span class="line">comedy_title_vectorized = vectorizer_comedy_title.fit_transform(comedy_title)</span><br><span class="line">lda_comedy_title_vectorized= LatentDirichletAllocation(n_topics=<span class="number">7</span>, max_iter=<span class="number">5</span>, learning_method=<span class="string">'online'</span>,verbose=<span class="literal">True</span>)</span><br><span class="line">comedy_title_vectorized_lda = lda_comedy_title_vectorized.fit_transform(comedy_title_vectorized )</span><br><span class="line"></span><br><span class="line">dash = pyLDAvis.sklearn.prepare(lda_comedy_title_vectorized,comedy_title_vectorized, vectorizer_comedy_title , mds=<span class="string">'tsne'</span>)</span><br><span class="line">pyLDAvis.show(dash)</span><br></pre></td></tr></table></figure><p>​    </p>]]></content>
    
    <summary type="html">
    
      文本分析的案例系列
    
    </summary>
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/categories/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
      <category term="话题挖掘" scheme="http://Raina-fighting.github.io/categories/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/%E8%AF%9D%E9%A2%98%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="python" scheme="http://Raina-fighting.github.io/tags/python/"/>
    
      <category term="文本分析" scheme="http://Raina-fighting.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
      <category term="话题挖掘" scheme="http://Raina-fighting.github.io/tags/%E8%AF%9D%E9%A2%98%E6%8C%96%E6%8E%98/"/>
    
      <category term="情感分析" scheme="http://Raina-fighting.github.io/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>hello-hexo</title>
    <link href="http://Raina-fighting.github.io/2019/04/03/hello-hexo/"/>
    <id>http://Raina-fighting.github.io/2019/04/03/hello-hexo/</id>
    <published>2019-04-03T10:09:58.000Z</published>
    <updated>2019-08-08T08:54:27.384Z</updated>
    
    <content type="html"><![CDATA[<p>Hello,Hexo.<br>Hello,April.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hello,Hexo.&lt;br&gt;Hello,April.&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
</feed>
